#!/usr/bin/env python3
"""
MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

4æŠæ³•ä»¤ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆe-Gov APIçµ±åˆï¼‰ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
æ—¢å­˜ã®Vectorãƒ™ãƒ¼ã‚¹è©•ä¾¡ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§çµæœã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
"""
import argparse
import json
import sys
import os
import time
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm
import re
from datetime import datetime
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
project_root = Path(__file__).parent.parent
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)

from app.core.rag_config import load_config as load_base_config
from app.retrieval.vector_retriever import VectorRetriever
from app.retrieval.bm25_retriever import BM25Retriever
from app.retrieval.hybrid_retriever import HybridRetriever

# MCPãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import importlib
mcp_module = importlib.import_module('examples.02_mcp_egov_agent')
MCPEgovPipeline = mcp_module.MCPEgovPipeline
load_mcp_config = mcp_module.load_config


def create_retriever(config):
    """è¨­å®šã«åŸºã¥ã„ã¦Retrieverã‚’ä½œæˆ"""
    retriever_type = config.retriever.retriever_type
    index_path = Path(config.vector_store_path)
    
    if retriever_type == "vector":
        return VectorRetriever(
            embedding_model=config.embedding.model_name,
            index_path=str(index_path / "vector"),
            use_mmr=config.retriever.use_mmr,
            mmr_lambda=config.retriever.mmr_lambda,
            mmr_fetch_k_max=config.retriever.mmr_fetch_k_max
        )
    elif retriever_type == "bm25":
        return BM25Retriever(
            index_path=str(index_path / "bm25"),
            tokenizer=config.retriever.bm25_tokenizer
        )
    else:
        vector_retriever = VectorRetriever(
            embedding_model=config.embedding.model_name,
            index_path=str(index_path / "vector"),
            use_mmr=config.retriever.use_mmr,
            mmr_lambda=config.retriever.mmr_lambda,
            mmr_fetch_k_max=config.retriever.mmr_fetch_k_max
        )
        bm25_retriever = BM25Retriever(
            index_path=str(index_path / "bm25"),
            tokenizer=config.retriever.bm25_tokenizer
        )
        return HybridRetriever(
            vector_retriever,
            bm25_retriever,
            fusion_method=config.retriever.fusion_method,
            vector_weight=config.retriever.vector_weight,
            bm25_weight=config.retriever.bm25_weight,
            rrf_k=config.retriever.rrf_k,
            fetch_k_multiplier=config.retriever.fetch_k_multiplier
        )


def load_dataset(dataset_path: Path) -> List[Dict[str, Any]]:
    """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if isinstance(data, dict) and 'questions' in data:
        return data['questions']
    elif isinstance(data, list):
        return data
    else:
        raise ValueError(f"Unknown dataset format: {dataset_path}")


def create_multiple_choice_prompt(question: str, choices: str, context: str = "") -> str:
    """4æŠå•é¡Œç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    prompt = """ã‚ãªãŸã¯æ—¥æœ¬ã®æ³•å¾‹ã«ç²¾é€šã—ãŸæ³•å¾‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æ³•ä»¤æ¡æ–‡ã«åŸºã¥ã„ã¦ã€4æŠå•é¡Œã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€æ³•ä»¤æ¡æ–‡ã€‘
{context}

ã€å•é¡Œæ–‡ã€‘
{question}

ã€é¸æŠè‚¢ã€‘
{choices}

ã€æŒ‡ç¤ºã€‘
ä¸Šè¨˜ã®æ³•ä»¤æ¡æ–‡ã«åŸºã¥ã„ã¦ã€é¸æŠè‚¢aã€bã€cã€dã®ä¸­ã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªã‚‚ã®ã‚’1ã¤é¸ã‚“ã§ãã ã•ã„ã€‚
å›ç­”ã¯å¿…ãšã€Œaã€ã€Œbã€ã€Œcã€ã€Œdã€ã®ã„ãšã‚Œã‹1æ–‡å­—ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚

å›ç­”: """
    
    return prompt.format(context=context, question=question, choices=choices)


def extract_answer(response: str) -> str:
    """LLMå¿œç­”ã‹ã‚‰å›ç­”(a/b/c/d)ã‚’æŠ½å‡º"""
    response = response.strip().lower()
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å˜ç‹¬ã® a, b, c, d
    if response in ['a', 'b', 'c', 'd']:
        return response
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: "å›ç­”: a" ã‚„ "ç­”ãˆ: a" ãªã©ã®å½¢å¼
    match = re.search(r'[å›ç­”ç­”ãˆ][:ï¼š]\s*([abcd])', response)
    if match:
        return match.group(1)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æœ€åˆã«å‡ºç¾ã™ã‚‹ a, b, c, d
    match = re.search(r'\b([abcd])\b', response)
    if match:
        return match.group(1)
    
    # æŠ½å‡ºå¤±æ•—
    return ""


def evaluate_question(
    pipeline: MCPEgovPipeline,
    question_data: Dict[str, Any],
    force_api: bool = False,
    force_local: bool = False
) -> Dict[str, Any]:
    """
    1å•ã‚’è©•ä¾¡
    
    Returns:
        æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨äº’æ›æ€§ã®ã‚ã‚‹çµæœè¾æ›¸
    """
    question_text = question_data.get("question", "")
    choices = question_data.get("choices", {})
    correct_answer = question_data.get("answer", "").lower()
    question_id = question_data.get("id", "")
    
    # é¸æŠè‚¢ã®æ•´å½¢
    choices_text = "\n".join([
        f"{key}. {value}" for key, value in choices.items()
    ])
    
    start_time = time.time()
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å›ç­”ã‚’å–å¾—
        result = pipeline.query(
            question_text,
            force_api=force_api,
            force_local=force_local
        )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        contexts = result.get("contexts", [])
        context_text = "\n\n".join([
            f"[{i+1}] {ctx.get('law_title', 'ä¸æ˜')} ç¬¬{ctx.get('article', '?')}æ¡\n{ctx.get('text', '')}"
            for i, ctx in enumerate(contexts[:5])
        ])
        
        # 4æŠãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¦LLMã«é€ä¿¡
        prompt = create_multiple_choice_prompt(question_text, choices_text, context_text)
        response = pipeline.llm.invoke(prompt)
        
        # å›ç­”ã‚’æŠ½å‡º
        predicted_answer = extract_answer(str(response))
        
        response_time = time.time() - start_time
        
        # æ­£ç­”åˆ¤å®š
        is_correct = (predicted_answer == correct_answer)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®åˆ¤å®š
        source = result.get("source", "unknown")
        metadata = result.get("metadata", {})
        api_used = metadata.get("num_api_documents", 0) > 0
        
        return {
            "question_id": question_id,
            "question": question_text,
            "correct": is_correct,
            "predicted": predicted_answer,
            "ground_truth": correct_answer,
            "response_time": response_time,
            "mcp_source": source,  # MCPç‰¹æœ‰ã®æƒ…å ±
            "api_used": api_used,
            "num_api_docs": metadata.get("num_api_documents", 0),
            "num_local_docs": metadata.get("num_local_documents", 0),
            "error": None
        }
    
    except Exception as e:
        response_time = time.time() - start_time
        return {
            "question_id": question_id,
            "question": question_text,
            "correct": False,
            "predicted": "",
            "ground_truth": correct_answer,
            "response_time": response_time,
            "mcp_source": "error",
            "api_used": False,
            "num_api_docs": 0,
            "num_local_docs": 0,
            "error": str(e)
        }


def calculate_metrics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    total = len(results)
    correct = sum(1 for r in results if r["correct"])
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã®é›†è¨ˆ
    by_source = {}
    for r in results:
        source = r.get("mcp_source", "unknown")
        if source not in by_source:
            by_source[source] = {"total": 0, "correct": 0}
        by_source[source]["total"] += 1
        if r["correct"]:
            by_source[source]["correct"] += 1
    
    # ã‚½ãƒ¼ã‚¹åˆ¥ã®æ­£ç­”ç‡ã‚’è¨ˆç®—
    for source, stats in by_source.items():
        stats["accuracy"] = stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
    
    # APIä½¿ç”¨çŠ¶æ³
    api_used_count = sum(1 for r in results if r.get("api_used", False))
    api_success_count = sum(1 for r in results if r.get("api_used", False) and r["correct"])
    
    # å¹³å‡å¿œç­”æ™‚é–“
    avg_response_time = sum(r["response_time"] for r in results) / total if total > 0 else 0.0
    
    # ã‚¨ãƒ©ãƒ¼ç‡
    error_count = sum(1 for r in results if r.get("error") is not None)
    
    return {
        "accuracy": correct / total if total > 0 else 0.0,
        "correct": correct,
        "total": total,
        "avg_time": avg_response_time,
        "error_rate": error_count / total if total > 0 else 0.0,
        "by_source": by_source,
        "api_metrics": {
            "api_used_count": api_used_count,
            "api_success_count": api_success_count,
            "api_usage_rate": api_used_count / total if total > 0 else 0.0,
            "api_success_rate": api_success_count / api_used_count if api_used_count > 0 else 0.0
        }
    }


def main():
    parser = argparse.ArgumentParser(description="MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡")
    parser.add_argument(
        "--data",
        type=Path,
        default=project_root / "datasets" / "lawqa_jp" / "data" / "selection.json",
        help="è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("mcp_benchmark_results.json"),
        help="çµæœã®å‡ºåŠ›å…ˆ"
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["api_preferred", "local_preferred", "api_only", "local_only"],
        default="api_preferred",
        help="MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ (api_preferred: APIå„ªå…ˆ/ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯, local_preferred: ãƒ­ãƒ¼ã‚«ãƒ«å„ªå…ˆ, api_only: APIå¼·åˆ¶, local_only: ãƒ­ãƒ¼ã‚«ãƒ«å¼·åˆ¶)"
    )
    parser.add_argument(
        "--samples",
        type=int,
        default=None,
        help="è©•ä¾¡ã™ã‚‹å•é¡Œæ•°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"
    )
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ 4æŠå•é¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡")
    print("=" * 80)
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {args.data}")
    print(f"ãƒ¢ãƒ¼ãƒ‰: {args.mode}")
    print(f"å‡ºåŠ›å…ˆ: {args.output}")
    print()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    dataset = load_dataset(args.data)
    
    if args.samples:
        dataset = dataset[:args.samples]
        print(f"   å•é¡Œæ•°ã‚’{args.samples}ä»¶ã«åˆ¶é™")
    
    print(f"   ç·å•é¡Œæ•°: {len(dataset)}")
    print()
    
    # è¨­å®šã®èª­ã¿è¾¼ã¿
    print("âš™ï¸  è¨­å®šã‚’èª­ã¿è¾¼ã¿ä¸­...")
    base_config = load_base_config()
    mcp_config = load_mcp_config(validate=False)
    
    # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸè¨­å®š
    if args.mode == "local_preferred":
        mcp_config.prefer_api = False
    elif args.mode == "api_only":
        mcp_config.prefer_api = True
        mcp_config.fallback_to_local = False
    elif args.mode == "local_only":
        mcp_config.prefer_api = False
        mcp_config.fallback_to_local = False
    
    print(f"   APIå„ªå…ˆ: {mcp_config.prefer_api}")
    print(f"   ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {mcp_config.fallback_to_local}")
    print()
    
    # Retrieverã®ä½œæˆ
    print("ğŸ” Retrieverã‚’åˆæœŸåŒ–ä¸­...")
    retriever = create_retriever(base_config)
    print("   âœ“ RetrieveråˆæœŸåŒ–å®Œäº†")
    print()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ
    print("ğŸ¤– MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–ä¸­...")
    pipeline = MCPEgovPipeline(config=mcp_config, retriever=retriever)
    print("   âœ“ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
    print()
    
    # è©•ä¾¡ã®å®Ÿè¡Œ
    print("ğŸš€ è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...")
    print()
    
    force_api = (args.mode == "api_only")
    force_local = (args.mode == "local_only")
    
    results = []
    for question_data in tqdm(dataset, desc="è©•ä¾¡ä¸­", unit="å•"):
        result = evaluate_question(
            pipeline,
            question_data,
            force_api=force_api,
            force_local=force_local
        )
        results.append(result)
    
    # æŒ‡æ¨™ã®è¨ˆç®—
    print()
    print("=" * 80)
    print("ğŸ“Š è©•ä¾¡çµæœ")
    print("=" * 80)
    
    metrics = calculate_metrics(results)
    
    print(f"å…¨ä½“æ­£ç­”ç‡: {metrics['accuracy']:.2%}")
    print(f"ç·å•é¡Œæ•°: {metrics['total']}")
    print(f"æ­£ç­”æ•°: {metrics['correct']}")
    print(f"å¹³å‡å¿œç­”æ™‚é–“: {metrics['avg_time']:.2f}ç§’")
    print(f"ã‚¨ãƒ©ãƒ¼ç‡: {metrics['error_rate']:.2%}")
    print()
    
    print("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥æ­£ç­”ç‡:")
    for source, stats in metrics['by_source'].items():
        print(f"  {source}: {stats['accuracy']:.2%} ({stats['correct']}/{stats['total']})")
    print()
    
    if metrics['api_metrics']['api_used_count'] > 0:
        print("APIä½¿ç”¨çŠ¶æ³:")
        print(f"  APIä½¿ç”¨ç‡: {metrics['api_metrics']['api_usage_rate']:.2%}")
        print(f"  APIä½¿ç”¨å›æ•°: {metrics['api_metrics']['api_used_count']}")
        print(f"  APIçµŒç”±æ­£ç­”ç‡: {metrics['api_metrics']['api_success_rate']:.2%}")
        print()
    
    # çµæœã®ä¿å­˜
    output_data = {
        "evaluation_type": "mcp_agent",
        "mode": args.mode,
        "timestamp": datetime.now().isoformat(),
        "dataset": str(args.data),
        "metrics": metrics,
        "results": results
    }
    
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… çµæœã‚’ {args.output} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    print()


if __name__ == "__main__":
    main()
