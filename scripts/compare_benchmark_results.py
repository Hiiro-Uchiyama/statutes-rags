#!/usr/bin/env python3
"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœæ¯”è¼ƒãƒ„ãƒ¼ãƒ«

Vectorãƒ™ãƒ¼ã‚¹è©•ä¾¡ã¨MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã®çµæœã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
"""
import argparse
import json
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime


def load_result(filepath: Path) -> Dict[str, Any]:
    """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_metrics(result: Dict[str, Any]) -> Dict[str, Any]:
    """çµæœã‹ã‚‰ä¸»è¦æŒ‡æ¨™ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±ä¸€ï¼‰"""
    # evaluation_typeã§åˆ¤å®š
    eval_type = result.get("evaluation_type", "vector")
    
    if eval_type == "mcp_agent":
        # MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµæœ
        metrics = result.get("metrics", {})
        return {
            "type": "MCP Agent",
            "mode": result.get("mode", "unknown"),
            "accuracy": metrics.get("accuracy", 0.0),
            "correct": metrics.get("correct", 0),
            "total": metrics.get("total", 0),
            "avg_time": metrics.get("avg_time", 0.0),
            "error_rate": metrics.get("error_rate", 0.0),
            "api_usage": metrics.get("api_metrics", {}).get("api_usage_rate", 0.0),
            "by_source": metrics.get("by_source", {}),
            "timestamp": result.get("timestamp", "")
        }
    else:
        # Vectorãƒ™ãƒ¼ã‚¹çµæœ
        metrics = result.get("metrics", {})
        return {
            "type": "Vector-based",
            "mode": result.get("metadata", {}).get("retriever_type", "vector"),
            "accuracy": metrics.get("accuracy", 0.0),
            "correct": metrics.get("correct", 0),
            "total": metrics.get("total", 0),
            "avg_time": metrics.get("avg_time", 0.0),
            "error_rate": 0.0,  # Vectorãƒ™ãƒ¼ã‚¹ã«ã¯ã‚¨ãƒ©ãƒ¼ç‡ãŒãªã„
            "api_usage": 0.0,   # APIä½¿ç”¨ãªã—
            "by_source": {},
            "timestamp": result.get("metadata", {}).get("timestamp", "")
        }


def print_comparison(vector_metrics: Dict[str, Any], mcp_metrics: Dict[str, Any]):
    """æ¯”è¼ƒçµæœã‚’è¡¨ç¤º"""
    print("=" * 100)
    print("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœæ¯”è¼ƒ")
    print("=" * 100)
    print()
    
    # åŸºæœ¬æƒ…å ±
    print("ã€è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã€‘")
    print(f"  Vector-based: {vector_metrics['mode']} ãƒ¢ãƒ¼ãƒ‰")
    print(f"  MCP Agent:    {mcp_metrics['mode']} ãƒ¢ãƒ¼ãƒ‰")
    print()
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    print("ã€è©•ä¾¡æ—¥æ™‚ã€‘")
    print(f"  Vector-based: {vector_metrics['timestamp']}")
    print(f"  MCP Agent:    {mcp_metrics['timestamp']}")
    print()
    
    # ä¸»è¦æŒ‡æ¨™ã®æ¯”è¼ƒ
    print("=" * 100)
    print("ã€ä¸»è¦æŒ‡æ¨™ã®æ¯”è¼ƒã€‘")
    print("=" * 100)
    print()
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
    print(f"{'æŒ‡æ¨™':<20} {'Vector-based':<20} {'MCP Agent':<20} {'å·®åˆ†':<20}")
    print("-" * 100)
    
    # æ­£ç­”ç‡
    acc_diff = mcp_metrics['accuracy'] - vector_metrics['accuracy']
    acc_diff_str = f"{acc_diff:+.2%}"
    print(f"{'æ­£ç­”ç‡':<20} {vector_metrics['accuracy']:>18.2%} {mcp_metrics['accuracy']:>18.2%} {acc_diff_str:>20}")
    
    # æ­£ç­”æ•°
    correct_diff = mcp_metrics['correct'] - vector_metrics['correct']
    print(f"{'æ­£ç­”æ•°':<20} {vector_metrics['correct']:>18} {mcp_metrics['correct']:>18} {correct_diff:>+20}")
    
    # ç·å•é¡Œæ•°
    print(f"{'ç·å•é¡Œæ•°':<20} {vector_metrics['total']:>18} {mcp_metrics['total']:>18} {'-':>20}")
    
    # å¹³å‡å¿œç­”æ™‚é–“
    time_diff = mcp_metrics['avg_time'] - vector_metrics['avg_time']
    time_diff_str = f"{time_diff:+.2f}ç§’"
    print(f"{'å¹³å‡å¿œç­”æ™‚é–“':<20} {vector_metrics['avg_time']:>16.2f}ç§’ {mcp_metrics['avg_time']:>16.2f}ç§’ {time_diff_str:>20}")
    
    # ã‚¨ãƒ©ãƒ¼ç‡
    print(f"{'ã‚¨ãƒ©ãƒ¼ç‡':<20} {vector_metrics['error_rate']:>18.2%} {mcp_metrics['error_rate']:>18.2%} {'-':>20}")
    
    print()
    
    # MCPç‰¹æœ‰ã®æƒ…å ±
    if mcp_metrics['api_usage'] > 0:
        print("=" * 100)
        print("ã€MCP Agent ç‰¹æœ‰æƒ…å ±ã€‘")
        print("=" * 100)
        print(f"APIä½¿ç”¨ç‡: {mcp_metrics['api_usage']:.2%}")
        print()
        
        if mcp_metrics['by_source']:
            print("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥æ­£ç­”ç‡:")
            for source, stats in mcp_metrics['by_source'].items():
                accuracy = stats.get('accuracy', 0.0)
                correct = stats.get('correct', 0)
                total = stats.get('total', 0)
                print(f"  {source:<15}: {accuracy:.2%} ({correct}/{total})")
            print()
    
    # çµè«–
    print("=" * 100)
    print("ã€çµè«–ã€‘")
    print("=" * 100)
    
    if acc_diff > 0.01:
        improvement = "å‘ä¸Š"
        symbol = "ğŸ“ˆ"
    elif acc_diff < -0.01:
        improvement = "ä½ä¸‹"
        symbol = "ğŸ“‰"
    else:
        improvement = "ã»ã¼åŒç­‰"
        symbol = "â¡ï¸"
    
    print(f"{symbol} MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€Vectorãƒ™ãƒ¼ã‚¹ã¨æ¯”è¼ƒã—ã¦æ­£ç­”ç‡ãŒ {abs(acc_diff):.2%} {improvement}ã—ã¾ã—ãŸã€‚")
    print()
    
    if time_diff > 0:
        print(f"â±ï¸  å¿œç­”æ™‚é–“ã¯ {time_diff:.2f}ç§’ é…ããªã‚Šã¾ã—ãŸï¼ˆAPIå‘¼ã³å‡ºã—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰ã€‚")
    else:
        print(f"âš¡ å¿œç­”æ™‚é–“ã¯ {abs(time_diff):.2f}ç§’ é€Ÿããªã‚Šã¾ã—ãŸã€‚")
    print()


def save_comparison_report(
    vector_metrics: Dict[str, Any],
    mcp_metrics: Dict[str, Any],
    output_path: Path
):
    """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§ä¿å­˜"""
    report = {
        "comparison_timestamp": datetime.now().isoformat(),
        "vector_based": vector_metrics,
        "mcp_agent": mcp_metrics,
        "comparison": {
            "accuracy_diff": mcp_metrics['accuracy'] - vector_metrics['accuracy'],
            "time_diff": mcp_metrics['avg_time'] - vector_metrics['avg_time'],
            "mcp_advantage": mcp_metrics['accuracy'] > vector_metrics['accuracy']
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    print()


def main():
    parser = argparse.ArgumentParser(description="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœæ¯”è¼ƒãƒ„ãƒ¼ãƒ«")
    parser.add_argument(
        "--vector",
        type=Path,
        default=Path("evaluation_results_final.json"),
        help="Vectorãƒ™ãƒ¼ã‚¹è©•ä¾¡çµæœã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--mcp",
        type=Path,
        default=Path("mcp_benchmark_results.json"),
        help="MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡çµæœã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("benchmark_comparison.json"),
        help="æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›å…ˆ"
    )
    
    args = parser.parse_args()
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    print("ğŸ“¥ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    if not args.vector.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: Vectorãƒ™ãƒ¼ã‚¹è©•ä¾¡çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.vector}")
        print("   å…ˆã« Vector-based è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("   ./scripts/evaluate.sh 50")
        return 1
    
    if not args.mcp.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.mcp}")
        print("   å…ˆã« MCP Agent è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("   python scripts/evaluate_mcp_benchmark.py --samples 50")
        return 1
    
    vector_result = load_result(args.vector)
    mcp_result = load_result(args.mcp)
    
    print("   âœ“ èª­ã¿è¾¼ã¿å®Œäº†")
    print()
    
    # æŒ‡æ¨™ã®æŠ½å‡º
    vector_metrics = extract_metrics(vector_result)
    mcp_metrics = extract_metrics(mcp_result)
    
    # æ¯”è¼ƒçµæœã®è¡¨ç¤º
    print_comparison(vector_metrics, mcp_metrics)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜
    save_comparison_report(vector_metrics, mcp_metrics, args.output)
    
    return 0


if __name__ == "__main__":
    exit(main())
