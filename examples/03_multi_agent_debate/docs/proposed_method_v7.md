# 提案手法 v7: 難易度適応型マルチパス法令QA

## 概要

v7は、問題の難易度を事前判定し、適切な処理パスを選択することで法令QAの精度を向上させる手法です。
v4（73.6%）をベースに、難易度に応じた処理分岐と質問タイプ別の明確な指示を追加しました。

---

## 研究背景・目的

### 背景

大規模言語モデル（LLM）の発展により、法律分野でのAI活用が注目されています。
しかし、法令QA（法律に関する質問応答）は以下の理由から依然として困難なタスクです：

1. **高い正確性要求**: 法的判断には厳密な条文解釈が必要
2. **専門知識の必要性**: 法令用語や条文構造の理解が必要
3. **Hallucination問題**: LLMが根拠のない回答を生成するリスク
4. **複雑な参照関係**: 「政令で定める」等の他法令への参照

### 目的

本研究では、**RAG（Retrieval-Augmented Generation）** と **マルチエージェント手法** を組み合わせ、
日本の法令QAタスクにおいて **80%以上の精度** を達成することを目指します。

### 研究の位置づけ

| 観点 | 本研究のアプローチ |
|------|-------------------|
| ドメイン | 日本の金融商品取引法（専門法令） |
| 手法 | RAG + 難易度適応型マルチパス |
| LLM | ローカル実行可能な8Bモデル（qwen3:8b） |
| 評価 | 140問の4択問題（選択式法令試験形式） |

---

## 日本の法令データ特有の課題

日本語法令には、英語法令とは異なる固有の課題が存在します。

### 1. 漢数字表記

条文番号や数値が漢数字で表記されるため、検索時のマッチングが困難。

```
例: 「第二十七条の二十三の三」「三分の二以上」「六十日以内」
```

**対策**: クエリ正規化モジュールでアラビア数字→漢数字変換を実施

```python
# 変換例
"第21条" → "第二十一条"
"第27条の5" → "第二十七条の五"
```

### 2. 複雑な条文参照構造

日本の法令は、本法→施行令→施行規則の階層構造を持ち、相互参照が多い。

```
例: 「政令で定めるところにより」→ 施行令を参照
    「内閣府令で定める」→ 施行規則を参照
```

**課題**: 質問に答えるために複数の法令文書を参照する必要がある

### 3. 否定形問題の多さ

日本の法令試験では「誤っているものを選べ」形式が多い。

| 問題タイプ | 説明 | 難易度 |
|------------|------|--------|
| 誤り選択 | 条文と異なるものを選ぶ | 高（反転思考が必要） |
| 正しい選択 | 条文と一致するものを選ぶ | 中 |
| 組み合わせ | 複数条件の組み合わせ | 高 |

**対策**: 質問タイプを判定し、タイプに応じた明確な指示をLLMに提供

### 4. 数値条件の厳密性

期間、割合、金額等の数値条件が厳密に定められている。

```
例: 「30日以内」vs「60日以内」、「過半数」vs「三分の二以上」
```

**対策**: 数値対比表を生成し、選択肢と条文の数値を明示的に比較

### 5. 長文・複雑な文構造

日本語法令は一文が非常に長く、係り受けが複雑。

```
例: 「〜の場合において、〜であるときは、〜を除き、〜するものとする。ただし、〜」
```

**課題**: LLMのコンテキスト長の制限、重要部分の抽出が困難

---

### 最終精度

| 指標 | 値 |
|------|-----|
| **全体精度** | **107/140 (76.4%)** |
| simple問題 | 97/121 (80.2%) |
| moderate問題 | 9/16 (56.2%) |
| complex問題 | 1/3 (33.3%) |

### 主要な特徴

1. **難易度事前判定**: SIMPLE / MODERATE / COMPLEX の3段階分類
2. **適応的処理パス**: 難易度に応じた最適な処理を選択
3. **質問タイプ別指示**: 「誤り選択」「正しい選択」等に応じた明確な指示
4. **数値対比表**: 選択肢と条文の数値を抽出・比較
5. **Hybrid検索**: Vector + BM25 (RRF融合)

---

## アーキテクチャ

```
[質問入力]
    │
    v
[クエリ正規化] ─── 条文番号の漢数字変換
    │
    v
[Hybrid検索] ─── Vector + BM25 (RRF融合, top_k=30)
    │
    v
[質問タイプ判定] ─── 誤り選択/正しい選択/組み合わせ/単純選択
    │
    v
[難易度事前判定] ─── 複雑度スコアに基づく3段階分類
    │
    ├── SIMPLE (121問, 86%)
    │       │
    │       v
    │   [Simple Path] ─── 1回のLLM呼び出しで直接回答
    │
    ├── MODERATE (16問, 11%)
    │       │
    │       v
    │   [Moderate Path] ─── RetrieverAgent分析 → IntegratorAgent判断
    │
    └── COMPLEX (3問, 2%)
            │
            v
        [Complex Path] ─── 選択肢独立分析 → 統合判断
    │
    v
[最終回答]
```

---

## 難易度判定アルゴリズム

### 複雑度スコア算出

```python
complexity_score = 0

# 高複雑度キーワード（各+2）
if '政令で定める' in text: complexity_score += 2
if '施行令' in text: complexity_score += 2
if '内閣府令' in text: complexity_score += 2
if '準用' in text: complexity_score += 2
if '組み合わせ' in text: complexity_score += 2

# 中複雑度キーワード（各+1）
if '誤っている' in text: complexity_score += 1
if '正しくない' in text: complexity_score += 1
if '以内' in text: complexity_score += 1
if '以上' in text: complexity_score += 1
if '未満' in text: complexity_score += 1
if '超える' in text: complexity_score += 1
if 'ただし' in text: complexity_score += 1

# 数値の多さ（+1〜+2）
numbers = extract_numbers(text)
if len(numbers) >= 3: complexity_score += 2
elif len(numbers) >= 1: complexity_score += 1

# 組み合わせ問題（+3）
if '組み合わせ' in question: complexity_score += 3
```

### 判定閾値

| 難易度 | スコア | 処理パス |
|--------|--------|----------|
| SIMPLE | 0-2 | Simple Path |
| MODERATE | 3-4 | Moderate Path |
| COMPLEX | 5+ | Complex Path |

---

## 処理パス詳細

### 1. Simple Path（単純問題用）

1回のLLM呼び出しで直接回答。過剰な分析を避け、精度低下を防止。

**精度**: 97/121 (80.2%)

#### プロンプト構造

```
あなたは法令QAの専門家です。

【検索された条文】
{context}

【問題】
{question}

【選択肢】
{choices}

{質問タイプ別指示}
- 「誤り選択」→「条文と異なる選択肢を選んでください」
- 「正しい選択」→「条文と一致する選択肢を選んでください」

各選択肢を条文と照合し、回答を決定してください。
最後に a, b, c, d のいずれか1文字のみで回答してください。

回答:
```

### 2. Moderate Path（中程度問題用）

2エージェント構成（v4スタイル）で段階的に処理。

**精度**: 9/16 (56.2%)

#### 処理フロー

```
[RetrieverAgent]
    │  - 各選択肢を条文と照合
    │  - 数値対比表を参照
    │  - 一致/不一致を判定
    │
    v
[IntegratorAgent]
    │  - 分析結果を統合
    │  - 質問タイプを考慮
    │  - 最終判断
    │
    v
[回答]
```

### 3. Complex Path（複雑問題用）

各選択肢を独立に分析し、最後に統合判断。

**精度**: 1/3 (33.3%)

#### 処理フロー

```
[選択肢a] → 条文照合 → 判定（一致/不一致/不明）
[選択肢b] → 条文照合 → 判定
[選択肢c] → 条文照合 → 判定
[選択肢d] → 条文照合 → 判定
    │
    v
[統合判断] → 質問タイプに応じて最適な選択肢を決定
```

---

## 数値対比表

選択肢と条文に含まれる数値を抽出し、比較を容易にする。

### 抽出パターン

```python
NUMBER_PATTERNS = [
    r'\d+[日月年週]',           # 期間
    r'[一二三四五六七八九十百千]+[日月年週]',
    r'\d+分の\d+',              # 分数
    r'\d+パーセント|\d+%',      # 割合
    r'\d+円|\d+万円',           # 金額
    r'\d+人',                   # 人数
    r'過半数|三分の二',         # 特殊表現
    r'\d+条|第[一二三四五六七八九十百]+条',  # 条文番号
]
```

### 対比表の例

```
【選択肢の数値】
  a: 30日, 過半数
  b: 60日, 三分の二
  c: (数値なし)
  d: 14日

【条文に含まれる数値】
  30日, 三分の二, 第二十四条
```

---

## 実験結果

### バージョン比較

| バージョン | 精度 | 特徴 |
|------------|------|------|
| v1 (Baseline) | 96/140 (68.6%) | 単純RAG + CoT |
| v4 | 103/140 (73.6%) | 2エージェント構成 |
| v7a (complex廃止) | 101/140 (72.1%) | complex→moderate統合（逆効果） |
| **v7b** | **107/140 (76.4%)** | **難易度適応 + 質問タイプ別指示** |

### v7b詳細結果（140問）

| 難易度 | 正解/総数 | 精度 |
|--------|----------|------|
| SIMPLE | 97/121 | **80.2%** |
| MODERATE | 9/16 | 56.2% |
| COMPLEX | 1/3 | 33.3% |

### 質問タイプ別（参考）

| タイプ | 説明 |
|--------|------|
| 誤り選択 | 「誤っているもの」を選ぶ問題 |
| 正しい選択 | 「正しいもの」を選ぶ問題 |
| 組み合わせ | 複数条件の組み合わせ問題 |
| 単純選択 | 上記以外の問題 |

---

## 失敗分析

### 主な失敗パターン

1. **検索精度の問題**
   - 必要な条文が検索結果に含まれない
   - 関連条文が多すぎてノイズになる

2. **数値比較の困難**
   - 微妙な数値の違いを見逃す
   - 「以上」「超える」等の境界条件

3. **複数条文の参照関係**
   - 「政令で定める」→施行令への参照
   - 準用規定の解釈

4. **LLMのランダム性**
   - 同じ問題でも結果が変動する

### 改善を試みた施策と結果

| 施策 | 結果 |
|------|------|
| complex処理の廃止（v7a） | 逆効果（-3.6%） |
| moderate強化プロンプト | 逆効果（-12%） |
| Self-Consistency（多数決） | 処理時間の問題で中止 |

**結論**: 過剰な処理は精度を低下させる。シンプルな構成が最も効果的。

---

## 実装ファイル

| ファイル | 役割 |
|----------|------|
| `proposed_workflow_v7.py` | v7ワークフロー本体 |
| `run_proposed_v7.py` | テスト実行スクリプト |
| `test_moderate_only.py` | moderate問題のみのテストスクリプト |
| `app/utils/number_normalizer.py` | 条文番号正規化 |

---

## 設定パラメータ

```python
WorkflowConfig(
    llm_model="qwen3:8b",     # LLMモデル
    timeout=180,               # タイムアウト（秒）
    num_ctx=16000,             # コンテキスト長
    top_k=30                   # 検索件数
)
```

---

## 今後の改善方向

### 1. 検索精度の向上

- **Reranker導入**: 検索結果を再ランキング（ただし過去の実験では逆効果）
- **クエリ拡張**: 関連キーワードの追加

### 2. LLMの変更

- **より大きなモデル**: qwen3:14b, qwen3:32b
- **専門モデル**: 法令特化のファインチューニング

### 3. アンサンブル手法

- **Self-Consistency**: 複数回推論の多数決（処理時間との兼ね合い）
- **異なるプロンプト**: 複数のプロンプトで推論し統合

### 4. エラー分析の深化

- 失敗パターンの詳細分類
- パターン別の対策実装

---

## 結論

v7bは、難易度事前判定と質問タイプ別の明確な指示により、v4（73.6%）から**76.4%（+2.8%）**に精度を向上させました。

**主な成果**:
- simple問題で**80%超え**を達成
- 過剰処理による精度低下を回避
- 質問タイプに応じた適切な指示

**残課題**:
- moderate/complex問題の精度向上（56.2%, 33.3%）
- 80%目標には+5問の改善が必要

---

## 学術的貢献と新規性

### 核心的主張: "Less is More" + 難易度判断エージェント

本研究の最も重要な発見は以下の2点です：

> **1. シンプルな構成が最高の精度を達成する**
> 
> **2. 難易度判断エージェントを追加するだけで性能が向上する**

```
┌─────────────────────────────────────────────────────────────┐
│  従来手法: 全問題に同じ複雑な処理を適用                      │
│  → 単純問題で過剰処理による精度低下                         │
│                                                             │
│  提案手法: 難易度判断エージェントで処理を分岐               │
│  → 問題特性に応じた最適な処理を選択                         │
│  → シンプルな問題にはシンプルな処理                         │
│  → 複雑な問題にのみ多段階処理                               │
└─────────────────────────────────────────────────────────────┘
```

### 1. 難易度判断エージェントの有効性

**LLMに「難しさを判断するエージェント」を追加するだけで性能向上**

| 観点 | 説明 |
|------|------|
| **問題の本質** | 全ての問題に同じ処理は非効率かつ逆効果 |
| **解決策** | 難易度を事前判定し、適切な処理パスを選択 |
| **効果** | 過剰処理による精度低下を防止（v4: 73.6% → v7: 76.4%） |

```python
# 難易度判断エージェントの疑似コード
difficulty = DifficultyJudgeAgent.assess(question, choices)

if difficulty == SIMPLE:
    return SimpleAgent.answer(question)      # 1回のLLM呼び出し
elif difficulty == MODERATE:
    return ModerateAgent.answer(question)    # 2段階処理
else:
    return ComplexAgent.answer(question)     # 多段階処理
```

### 2. 日本語法令データ特有の処理

日本語法令には、英語法令にはない固有の課題があり、専用の処理が必要：

| 課題 | 本研究での対策 |
|------|---------------|
| **漢数字表記** | クエリ正規化モジュール（第21条→第二十一条） |
| **複雑な参照関係** | 本法・施行令・施行規則の階層検索 |
| **否定形問題** | 質問タイプ判定と明示的指示 |
| **数値条件の厳密性** | 数値対比表による構造化比較 |

### 3. Hallucination問題への対処

**条文と選択肢の構造化による根拠明示**

```
┌─────────────────────────────────────────────────────────────┐
│  問題: LLMが根拠のない回答を生成するリスク                  │
│                                                             │
│  対策1: 検索された条文のみを参照させる（RAG）              │
│  対策2: 数値対比表で条文の数値を明示                        │
│  対策3: 質問タイプに応じた明確な判断基準を提示              │
└─────────────────────────────────────────────────────────────┘
```

### 4. 質問タイプ別指示の効果

「誤り選択」「正しい選択」等の質問タイプをLLMに明示することで、回答精度が向上。

```
改善前: 「最も適切な選択肢を選んでください」
改善後: 「この問題は誤っているものを選ぶ問題です。条文と異なる選択肢を選んでください」
```

### 5. ローカルLLMの実用性検証

商用APIに依存せず、ローカル実行可能な8Bモデル（qwen3:8b）で実用的な精度を達成。

| 観点 | 評価 |
|------|------|
| プライバシー | 機密性の高い法的文書をローカル処理可能 |
| コスト | API利用料不要 |
| レイテンシ | ネットワーク遅延なし |

### 6. 失敗分析からの "Less is More" 原則

追加機能が必ずしも精度向上につながらないことを実験的に確認。

| 試みた施策 | 結果 | 知見 |
|------------|------|------|
| Reranker追加 | 逆効果 | LLMの混乱を招く |
| プロンプト詳細化 | 逆効果 | 過剰な指示は有害 |
| Self-Consistency | 時間増大 | コスト対効果が低い |
| **シンプルな構成** | **最良** | **Less is More** |

---

## 発展構想: v9 - 構造化法令QAシステム

v7の知見を活かし、80%超えを目指すv9の設計構想：

### 核心アイデア: 条文の関係性を構造化して保存

```
┌─────────────────────────────────────────────────────────────┐
│                    v9 アーキテクチャ                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  [質問入力]                                                 │
│      │                                                      │
│      v                                                      │
│  ┌──────────────────┐                                       │
│  │ 難易度判断Agent  │ ← v7で効果実証済み                   │
│  └──────────────────┘                                       │
│      │                                                      │
│      v                                                      │
│  ┌──────────────────┐   ┌──────────────────────────────┐   │
│  │ 検索Agent        │──→│ 構造化法令DB                 │   │
│  └──────────────────┘   │  - 条文テキスト              │   │
│      │                  │  - 数値情報（期間・割合等）  │   │
│      │                  │  - 参照関係グラフ            │   │
│      v                  │  - 条文間の関係性            │   │
│  ┌──────────────────┐   └──────────────────────────────┘   │
│  │ 分析Agent        │                                       │
│  └──────────────────┘                                       │
│      │                                                      │
│      v                                                      │
│  ┌──────────────────┐                                       │
│  │ 検証Agent        │ ← 判断根拠の擦り合わせ               │
│  └──────────────────┘                                       │
│      │                                                      │
│      v                                                      │
│  [最終回答 + 根拠]                                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 3体のエージェント構成

| Agent | 役割 | v7からの進化 |
|-------|------|-------------|
| **難易度判断Agent** | 問題の複雑度を判定 | v7で実証済み、継続使用 |
| **分析Agent** | 構造化データを用いた条文分析 | 数値・参照関係の構造化 |
| **検証Agent** | 判断根拠の確認・擦り合わせ | Hallucination防止 |

### 構造化法令DB

日本語法令データの情報を解釈して構造化保存：

```python
# 構造化法令データの例
{
    "article_id": "第二十七条の二十三の三",
    "text": "...",
    "numbers": {
        "期間": ["六十日以内", "三十日以内"],
        "割合": ["三分の二以上", "過半数"],
        "金額": ["一億円以上"]
    },
    "references": [
        {"type": "施行令", "target": "第十四条の三の七"},
        {"type": "内閣府令", "target": "第十一条の三"}
    ],
    "related_articles": ["第二十七条の二十三の二", "第二十七条の二十三の四"]
}
```

### 判断根拠の擦り合わせ

```
┌─────────────────────────────────────────────────────────────┐
│  分析Agent: 「選択肢aは第27条の23の3に基づき正しい」        │
│                    ↓                                        │
│  検証Agent: 「第27条の23の3の内容を確認:                    │
│              - 期間: 60日以内 ← 選択肢aの記載と一致          │
│              - 適用条件: 上場会社 ← 問題文の条件と一致       │
│              → 判断根拠は妥当」                              │
│                    ↓                                        │
│  最終回答: a（根拠: 第27条の23の3、期間60日の一致を確認）   │
└─────────────────────────────────────────────────────────────┘
```

### 検索精度向上案

| アプローチ | 説明 | 期待効果 |
|------------|------|---------|
| **条文グラフ検索** | 参照関係を辿って関連条文を取得 | 複雑な参照関係への対応 |
| **数値特化検索** | 期間・割合等の数値条件でフィルタ | 数値問題の精度向上 |
| **階層検索** | 本法→施行令→施行規則を連鎖検索 | 「政令で定める」への対応 |
| **クエリ拡張** | 同義語・関連条文の自動追加 | 検索漏れ防止 |

### v9の期待効果（実験前）

| 課題 | v7での状況 | v9での対策 |
|------|-----------|-----------|
| 複雑な参照関係 | 検索で一部対応 | 参照グラフで完全対応 |
| Hallucination | RAGで軽減 | 検証Agentで根拠確認 |
| 数値条件の比較 | 数値対比表 | 構造化DBで厳密比較 |
| moderate問題 | 56.2% | 構造化+検証で80%目標 |

---

## v9実験結果と失敗分析

### 実験結果

v9（構造化法令DB + 検証Agent）を実装し、140問テストを実施。

| 指標 | v9 | v7 | 差 |
|------|-----|-----|-----|
| **全体** | **95/140 (67.9%)** | 107/140 (76.4%) | **-8.5%** |
| simple | 73/100 (73.0%) | 97/121 (80.2%) | -7.2% |
| moderate | 17/29 (58.6%) | 9/16 (56.2%) | +2.4% |
| complex | 5/11 (45.5%) | 1/3 (33.3%) | +12.2% |

### 失敗の原因分析

#### 1. 検証Agentの過剰処理

```
期待: 分析Agent → 検証Agent → より正確な回答
実際: 分析Agent → 検証Agent → 迷いによる誤答増加
```

検証Agentが分析結果を「再検討」することで、元々正しかった判断を覆すケースが多発。

**具体例**:
- 分析Agent: 「選択肢bが正しい」（正解）
- 検証Agent: 「念のため確認... やはりcかもしれない」（誤答）

#### 2. simple問題での大幅劣化

| 難易度 | v7 | v9 | 影響 |
|--------|-----|-----|------|
| simple | 80.2% | 73.0% | **-7.2%** |

simple問題は「1回のLLM呼び出しで直接回答」が最適。
検証Agentを通すことで、不要な「迷い」が生じた。

#### 3. 構造化DBの効果限定的

構造化法令DBは数値対比表の強化版として機能したが、
検証Agentとの組み合わせで逆効果に。

### 得られた知見

> **"Less is More" 原則の再確認**
>
> 追加の検証ステップは、信頼性向上ではなく精度低下を招いた。

| 知見 | 詳細 |
|------|------|
| **検証Agentは不要** | simple/moderate問題では逆効果 |
| **構造化DBは有効** | ただし検証Agentなしで使うべき |
| **難易度判断が最重要** | 処理の複雑さより、適切な分岐が効果的 |

### v9の教訓まとめ

```
┌─────────────────────────────────────────────────────────────┐
│  v9の失敗から学んだこと:                                     │
│                                                             │
│  1. 検証Agentは「二重チェック」ではなく「迷いの増幅」       │
│  2. LLMは一度の判断が最も正確（再考は精度低下）             │
│  3. 追加処理 ≠ 精度向上                                     │
│  4. 難易度判断Agent単体が最も効果的                         │
│                                                             │
│  結論: v7（シンプル構成）が最適解                           │
└─────────────────────────────────────────────────────────────┘
```

### 今後の改善方向

v9の失敗を踏まえ、以下の方向で改善を検討：

| アプローチ | 期待効果 | リスク |
|------------|---------|--------|
| **LLMの強化** (qwen3:14b) | 基本性能向上 | 処理速度低下 |
| **検索精度向上** | より適切な条文取得 | 複雑化リスク |
| **プロンプト微調整** | 誤答パターン対策 | 過適合リスク |

---

## 関連研究

### RAG (Retrieval-Augmented Generation)

- Lewis et al. (2020): RAGの提案
- 本研究: Hybrid検索（Vector + BM25）によるRAGの強化

### Legal AI

- 法律分野でのNLP応用研究
- 本研究: 日本語法令への特化

### Multi-Agent Systems

- LLMを用いたマルチエージェント手法
- 本研究: 難易度に応じた適応的エージェント選択

### Knowledge Graphs in Legal Domain

- 法令知識グラフの構築と活用
- v9構想: 条文間の参照関係グラフ

---

## 参考文献

- v4ドキュメント: `docs/proposed_method_v4.md`
- 実験ログ: `results/proposed_v7_results.json`
