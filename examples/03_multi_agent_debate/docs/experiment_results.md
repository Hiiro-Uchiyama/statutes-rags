# 法令QA 実験結果レポート

## 実験概要

法令4択問題（LawQA-JP）データセットを用いた各種RAG手法の比較実験。

### データセット
- **名称**: LawQA-JP (selection.json)
- **問題数**: 140問（4択問題）
- **ソース**: e-Gov法令データ
- **インデックス文書数**: 6,287文書（QA関連法令）

### 共通設定
| パラメータ | 値 |
|-----------|-----|
| LLMモデル | `qwen3:8b` |
| Embedding | `intfloat/multilingual-e5-large` |
| Context長 | 30,000 tokens |
| Temperature | 0.1 |
| 検索候補数 (top_k) | 30 |
| コンテキスト使用数 | 15文書 |

---

## ベースライン手法

### 1. Vector検索のみ (Baseline)
**パイプライン**: `[質問] -> [Vector検索] -> [上位15件] -> [LLM回答]`

| 項目 | 設定 |
|------|------|
| 検索手法 | FAISS Vector検索 |
| Embedding | multilingual-e5-large |
| top_k | 30 (上位15件使用) |

**結果**:
- 正解数: 96/140
- **精度: 68.6%**
- 平均応答時間: 約9.5秒/問

---

### 2. Hybrid検索 (Vector + BM25 RRF)
**パイプライン**: `[質問] -> [Vector + BM25] -> [RRF統合] -> [上位15件] -> [LLM回答]`

| 項目 | 設定 |
|------|------|
| Vector検索 | FAISS |
| BM25検索 | MeCab tokenizer |
| Fusion | RRF (k=60) |
| fetch_k_multiplier | 2 |

**結果**:
- 正解数: 97/140
- **精度: 69.3%** (+0.7%)
- 平均応答時間: 約8秒/問

---

### 3. Vector + Reranker
**パイプライン**: `[質問] -> [Vector検索] -> [Cross-encoder Rerank] -> [上位15件] -> [LLM回答]`

| 項目 | 設定 |
|------|------|
| 検索手法 | FAISS Vector検索 |
| Reranker | cross-encoder/mmarco-mMiniLMv2-L12-H384-v1 |
| top_k | 30 |
| rerank_top_n | 15 |

**結果**:
- 正解数: 97/140
- **精度: 69.3%** (+0.7%)
- 平均応答時間: 約7秒/問

---

### 4. Hybrid + Reranker
**パイプライン**: `[質問] -> [Vector + BM25] -> [RRF統合] -> [Cross-encoder Rerank] -> [上位15件] -> [LLM回答]`

| 項目 | 設定 |
|------|------|
| Hybrid | Vector + BM25 RRF (k=60) |
| Reranker | cross-encoder/mmarco-mMiniLMv2-L12-H384-v1 |
| top_k | 30 |
| rerank_top_n | 15 |

**結果**:
- 正解数: 94/140
- **精度: 67.1%** (-1.5%)
- 平均応答時間: 約12秒/問

**注**: Hybrid + Rerankerの組み合わせは、HybridまたはReranker単体より低い精度となった。RRFとCross-encoderの両方でランキングを行うことで、オーバーフィッティングや冗長な再順序付けが発生した可能性がある。

---

## 提案手法

### 5. Multi-Agent + CoT共有 (Proposed)
**パイプライン**:
```
[質問] -> [RetrieverAgent] -> [Hybrid検索 + Query拡張]
                                    |
                                    v
              [InterpreterAgent] <- [検索結果 + retrieval_cot]
                    |
                    v
              [構造化分析: 質問タイプ特定 -> 選択肢-条文照合 -> 判断]
                    |
                    v
              [JudgeAgent] <- [解釈結果 + legal_cot]
                    |
                    v
              [検証・補強] -> [最終回答]
```

| 項目 | 設定 |
|------|------|
| 検索 | Hybrid (Vector + BM25 RRF) |
| Query拡張 | LLMによるキーワード・条文番号抽出 |
| エージェント | Retriever / Interpreter / Judge |
| CoT共有 | retrieval_cot -> legal_cot -> verified_cot |

**主な貢献**:
1. **構造化分析**: 質問タイプ特定 -> 選択肢ごとの条文照合 -> 証拠ベース判断
2. **Citation-Grounded CoT**: 各選択肢に対応する条文を明示的に引用
3. **検証ループ**: JudgeAgentによる解釈の妥当性検証

**結果 (Vector版)**:
- 正解数: 84/140
- **精度: 60.0%** (-8.6%)
- 平均応答時間: 約35秒/問

**問題点分析**:
| 問題 | 件数 | 説明 |
|------|------|------|
| 法令CoTが空 | 34問 (24%) | CoT生成に失敗 |
| 高確信度で不正解 | 40問 | 確信度>=0.9で誤答 |
| 平均確信度 | 0.90 | 確信度と精度の乖離 |

**誤答パターン**:
- 正解が `b`, `c` の問題で `d` を予測する傾向
- JudgeAgentがInterpreterの判断をほぼ修正していない

---

### 6. Multi-Agent + CoT共有 v2 (Proposed - Improved)
**パイプライン**:
```
[質問] -> [RetrieverAgent] -> [Hybrid検索]
                                    |
                                    v
              [InterpreterAgent] <- [検索結果 + 自然な思考]
                    |
                    v
              [JudgeAgent] <- [妥当性検証]
                    |
           ┌───────┴───────┐
           v               v
        [承認]         [要再検討] -> [議論ラウンド] -> [再検索・再考]
           |                                              |
           └──────────────┬───────────────────────────────┘
                          v
                    [最終回答]
```

| 項目 | 設定 |
|------|------|
| 検索 | Hybrid (Vector + BM25 RRF) |
| CoT生成 | 簡素化（自然な思考） |
| JudgeAgent | 妥当性検証に特化 |
| 議論ラウンド | 要再検討時に追加検索・再考 |
| 最大議論回数 | 2回 |

**結果**:
- 正解数: 85/140
- **精度: 60.7%** (-7.9% vs Baseline)
- 議論ラウンド発動: 26回 (18.6%の問題)
- 平均応答時間: 約40秒/問

**議論ラウンドの効果**:
- 発動率: 26/140問 (18.6%)
- 議論ラウンド問題の正解率: 分析中

---

## 結果サマリ

| 手法 | 正解数 | 精度 | 改善幅 |
|------|--------|------|--------|
| Vector (Baseline) | 96/140 | **68.6%** | - |
| Hybrid (Vector + BM25) | 97/140 | 69.3% | +0.7% |
| Vector + Reranker | 97/140 | 69.3% | +0.7% |
| Hybrid + Reranker | 94/140 | 67.1% | -1.5% |
| Proposed Vector (v1) | 84/140 | 60.0% | -8.6% |
| **Proposed v2 (Hybrid + Discussion)** | **85/140** | **60.7%** | **-7.9%** |

### 考察
1. **Hybrid検索**: BM25の追加により+0.7%の改善。キーワードマッチングが有効。
2. **Reranker**: Cross-encoderにより+0.7%の改善。Vectorと同等の効果。
3. **Hybrid + Reranker**: 精度低下(-1.5%)。2段階のランキングが逆効果の可能性。
4. **Proposed v1**: 精度低下(-8.6%)。CoT生成失敗(24%)と過信問題が主因。
5. **Proposed v2**: v1から+0.7pt改善するも、ベースラインには未到達(-7.9%)。

### v2改善の効果分析
| 改善施策 | 結果 | 評価 |
|----------|------|------|
| CoT簡素化 | 生成失敗率低下 | ✓ 改善 |
| Judge役割明確化 | 実質検証機能 | △ 限定的 |
| 議論ラウンド | 26回発動 (18.6%) | △ 効果検証要 |

### 提案手法が低精度な根本原因（仮説）
1. **マルチエージェント自体のオーバーヘッド**: 
   - 単一LLM回答 vs 3エージェント+議論でノイズ蓄積
   - シンプルなRAGが最も効果的な可能性

2. **LLM（qwen3:8b）の限界**:
   - 複雑な法令推論には不十分なモデルサイズ
   - より大きなモデル（70B等）での検証が必要

3. **検索品質の問題**:
   - 正しい条文が取得できていない可能性
   - 検索精度自体の改善が先決

4. **タスク特性との不整合**:
   - 4択問題には単純なRAGが最適
   - 複雑な法律相談等では提案手法が有効かもしれない

### 提案手法の課題
1. **CoT生成の複雑さ**: プロンプトが複雑すぎてLLMがCoTを生成できない
2. **過信問題**: 高確信度で不正解が多い（確信度と精度の乖離）
3. **3エージェントのオーバーヘッド**: 複数回LLM呼び出しでノイズ蓄積
4. **JudgeAgentの機能不全**: Interpreterの判断をほぼ修正していない

---

## 改善版 (v2)

### 設計変更

#### 1. CoTの「思考の流れ」としての共有
**変更前**: 構造化されたCoT形式を強制
```
法令CoT:
[推論チェーン]
1. 条文根拠: [引用ID]の[条項]は...
2. 選択肢照合: 選択肢[X]の...
3. 結論: 質問が...
```

**変更後**: 自然な思考をそのまま共有
```
私の考え:
（自然な思考過程を記述。なぜその答えを選んだかを説明）
```

#### 2. JudgeAgentの役割明確化
**変更前**: 自分でも答えを考えて検証 → 混乱
**変更後**: Interpreterの思考の「妥当性判断」に特化
- 思考の論理は通っているか？
- 条文の引用は正しいか？
- 質問タイプの判断は合っているか？

#### 3. 議論ラウンドの追加
**変更前**: 確信度ベースのフォールバック
**変更後**: JudgeAgentが「要再検討」と判断した場合
1. Judgeの指摘を含めて追加検索
2. Interpreterに再考を依頼（Judgeの思考を共有）
3. 最終検証

### 期待効果
| 問題 | 改善策 | 期待効果 |
|------|--------|----------|
| CoT生成失敗 (24%) | プロンプト簡素化 | 自然な思考を生成可能に |
| 25問の劣化 | 構造強制の廃止 | 正解が歪まなくなる |
| Judge機能不全 | 役割明確化 | 実質的な検証 |
| 31問の共通弱点 | 議論ラウンド | 追加検索で改善 |

### ワークフロー（改善版）
```
[質問] -> [RetrieverAgent] -> [検索 + 検索意図]
                                    |
                                    v
              [InterpreterAgent] <- [検索結果 + Retrieverの思考]
                    |
                    v
              [自然な思考で判断] -> [私の考え + 回答]
                    |
                    v
              [JudgeAgent] <- [Interpreterの思考]
                    |
                    v
              [妥当性判断]
                    |
           ┌───────┴───────┐
           v               v
        [承認]         [要再検討]
           |               |
           v               v
      [最終回答]    [議論ラウンド]
                          |
                          v
                   [追加検索 + 再考]
                          |
                          v
                   [最終検証・回答]
```

---

## 実験環境

| 項目 | 値 |
|------|-----|
| OS | Ubuntu Linux 6.8.0-87-generic |
| GPU | NVIDIA (CUDA対応) |
| LLM推論 | Ollama (localhost:11434) |
| Python | 3.x |

---

## ファイル構成

```
examples/03_multi_agent_debate/
├── run_baseline.py              # Vector検索ベースライン
├── run_baseline_hybrid.py       # Hybrid検索ベースライン
├── run_baseline_reranker.py     # Vector + Reranker
├── run_baseline_hybrid_reranker.py  # Hybrid + Reranker
├── run_140q_fast.py             # 提案手法
├── proposed_workflow.py         # Multi-Agent実装
├── agents/
│   ├── retriever_agent.py       # 検索エージェント
│   ├── interpreter_agent.py     # 解釈エージェント
│   └── judge_agent.py           # 検証エージェント
├── results/
│   ├── baseline_rag_results.json
│   ├── baseline_hybrid_results.json
│   ├── baseline_reranker_results.json
│   ├── baseline_hybrid_reranker_results.json
│   └── full_140q_fast_*.json
└── docs/
    └── experiment_results.md    # このファイル
```

---

## 更新履歴

- 2025-12-02: 初版作成、Vector Baseline完了 (68.6%)
- 2025-12-02: Hybrid Baseline完了 (69.3%)
- 2025-12-02: Vector + Reranker完了 (69.3%)
- 2025-12-02: Hybrid + Reranker完了 (67.1%)
- 2025-12-02: 全ベースライン実験完了
- 2025-12-02: Proposed Vector v1完了 (60.0%) - ベースライン下回る
- 2025-12-02: Proposed v2設計・実装（CoT簡素化、Judge役割明確化、議論ラウンド）
- 2025-12-03: **Proposed v2完了 (60.7%)** - 議論ラウンド26回発動、v1から+0.7pt改善

---

## 検索品質分析（2025-12-03追加）

### 発見された問題

| 問題 | 影響度 | 詳細 |
|------|--------|------|
| **条文番号の表記不一致** | 高 | クエリ「第21条」（アラビア数字）vs インデックス「第二十一条」（漢数字） |
| **必要文書がTOP-kに含まれない** | 高 | Q8等で第二十一条がTOP-15にヒットしない |
| **チャンキング粒度** | 中 | 複数条文が1チャンクに含まれる |

### ベースライン vs 提案手法の詳細比較
- 両方正解: 66問
- 両方不正解: **25問** (共通弱点 = 検索品質の問題)
- ベースラインのみ正解: **30問** (マルチエージェントで劣化)
- 提案手法のみ正解: 19問 (マルチエージェントで改善)

**共通弱点の問題番号**: [2, 3, 8, 13, 19, 23, 29, 30, 31, 37, 40, 45, 47, 49, 54, 63, 64, 69, 81, 83, 88, 93, 105, 114, 119]

---

## 今後の改善案

### 優先度: 高（検索品質改善）

#### 1. クエリ前処理: 数字表記の正規化
```python
# アラビア数字 → 漢数字変換
"第21条" → "第二十一条"
"第164条" → "第百六十四条"
```
- **期待効果**: BM25検索のマッチング精度向上
- **実装難易度**: 低
- **予想改善**: +3〜5%

#### 2. Query Expansion強化
現在のQuery Expansionに加え:
- 条文番号の両表記を自動追加
- 法令の正式名称・略称の展開
- 関連条文（参照先）の追加

#### 3. チャンキング戦略の見直し
- 条文単位でのチャンキング（現状: セクション単位）
- 条文間の参照関係をメタデータとして保持
- 階層構造の保持（章→節→条）

### 優先度: 中（マルチエージェント改善）

#### 4. マルチエージェントの簡素化
現状の問題:
- 3エージェント構成でノイズ蓄積
- 30問がマルチエージェント処理で劣化

改善案:
- 2エージェント構成（Retriever + Interpreter）への簡素化
- JudgeAgentを確信度<0.7の場合のみ発動

#### 5. LLMプロンプトの最適化
- より簡潔なプロンプト設計
- 法令特化のfew-shot例示
- 回答フォーマットの厳格化

### 優先度: 低（長期検討）

#### 6. より大きなLLMでの検証
- 70B級モデル（Qwen-72B等）での精度検証
- API版LLM（GPT-4、Claude）との比較

#### 7. タスク種別の検討
- 4択問題以外のタスク（自由回答、法律相談等）での評価
- 提案手法が有効なタスクタイプの特定

---

## 実装ロードマップ（推奨順序）

1. **数字正規化の実装** (1時間) → 即効性あり
2. **チャンキング改善** (4時間) → 根本的改善
3. **マルチエージェント簡素化** (2時間) → 劣化30問への対策
4. **Query Expansion強化** (2時間) → 検索精度向上

---

## アブレーション実験（2025-12-03追加）

### 実験設計

XMLインデックス（5,045条文）を使用し、プロンプト改善とRerankerの効果を検証。

| 実験 | スクリプト | 条件 | 目的 |
|------|-----------|------|------|
| Ablation 1 | `run_ablation_xml_basic.py` | XML v2 + Hybrid + 基本プロンプト | ベースライン |
| Ablation 2 | `run_ablation_xml_cot.py` | XML v2 + Hybrid + CoTプロンプト | プロンプト改善効果 |
| Ablation 3 | `run_ablation_xml_cot_rerank.py` | XML v2 + Hybrid + CoT + Reranker | Reranker追加効果 |

### CoTプロンプトの改善点

**基本プロンプト**:
```
回答は a, b, c, d のいずれか1文字のみで答えてください。
```

**CoTプロンプト（改善版）**:
```
【重要】この問題は「誤っているもの/正しいもの」を選ぶ問題です。

【回答手順】
1. 各選択肢(a,b,c,d)を1つずつ確認する
2. 各選択肢の内容が参考法令のどの条文に対応するか特定する
3. 選択肢の記述が条文と一致するか（数値、条件、期間など細部まで）確認する
4. 質問の指示（正しいもの/誤っているもの）に従い回答を選ぶ
```

### 問題分析（検索成功・LLM失敗）

Q17の例：検索は成功（第160条を取得）するもLLMが誤判断

| 問題 | 根本原因 |
|------|----------|
| 検索 | **成功** - 必要な条文を取得 |
| LLM判断 | **失敗** - 細かい数字・条件の違いを見分けられない |

**具体例（Q17）**:
- 条文: 「知つた時から**一年間**」
- 選択肢d（正解=誤り）: 「知ってから**２年**経過していても...」
- LLM: bを選択（誤り）

→ CoTプロンプトで選択肢ごとの照合を明示的に指示

### 結果

| 実験 | 正解数 | 精度 | 備考 |
|------|--------|------|------|
| Ablation 1 (Basic) | 21/40 | ~52% | 途中停止（原因分析完了） |
| **Ablation 2 (CoT)** | **96/140** | **68.6%** | 旧ベースラインと同等 |
| Ablation 3 (CoT+Rerank) | 実行中 | -% | - |

### Ablation 2 範囲別分析

| 範囲 | 正解数 | 精度 |
|------|--------|------|
| Q1-30 | 13/30 | 43.3% |
| Q31-60 | 24/30 | 80.0% |
| Q61-90 | 22/30 | 73.3% |
| Q91-120 | 21/30 | 70.0% |
| Q121-140 | 16/20 | 80.0% |

### 発見

1. **CoTプロンプトの効果**: 基本プロンプト（~52%）→ CoTプロンプト（68.6%）で**+16%以上の改善**
2. **Q1-30の難しさ**: 金融商品取引法関連の問題が集中、細かい数値比較が必要
3. **Q31-140の安定性**: 70%以上の精度を維持

---

## 検索・LLM性能詳細分析（2025-12-03追加）

### 全140問の分類

| カテゴリ | 件数 | 割合 |
|---------|------|------|
| 検索成功 & 正解 | 82 | 58.6% |
| **検索成功 & LLM失敗** | **39** | **27.9%** |
| 検索部分成功 | 3 | 2.1% |
| 検索失敗 & 正解 | 13 | 9.3% |
| 検索失敗 & LLM失敗 | 3 | 2.1% |

### 重要な結論

| 指標 | 値 | 評価 |
|------|-----|------|
| **検索成功率** | 86.4% (121/140) | 十分 |
| **検索成功時LLM正答率** | 67.8% (82/121) | **改善余地あり** |

### 問題の根本原因

**LLMの法律解釈・判断能力の限界**:
- 細かい数値の違い（三月 vs 六月）を見分けられない
- 複数条文（本法と施行令）の情報統合が困難
- 複雑な法律用語の理解不足

### Q1-30が特に低い理由（43.3%）

| 要因 | 詳細 |
|------|------|
| 主要法令 | 金融商品取引法（76%、13/17問） |
| 問題タイプ | 期間・金額等の数値比較が多い |
| 必要情報 | 本法と施行令の両方を参照する必要 |

### Q2の具体例

**問題**: 有価証券報告書の提出期限
- **選択肢b（正解）**: 外国会社 → 六月以内
- **選択肢c（誤り）**: 内国会社 → 六月以内

**条文**:
- 第24条: 「内国会社 → **三月**以内、外国会社 → 政令で定める期間」
- 施行令第3条の4: 「政令で定める期間 = **六月**」

**検索結果**: 両方の条文をTOP-15内に取得（検索成功）
**LLMの失敗**: 「三月」と「六月」の違いを正しく判断できず

### 実装ミスの確認

| 項目 | 状態 |
|------|------|
| 検索実装 | 正常 |
| プロンプト構築 | 正常 |
| 数字正規化 | 正常 |
| インデックス | 正常（5,045条文） |

**結論**: 実装ミスなし。問題はLLMの判断能力に起因。

---

## 提案手法 v3 設計

### マルチエージェント構成

```
[質問] 
   │
   v
[RetrieverAgent] ─── 検索（現状維持、86%成功）
   │
   v
[ChoiceVerifierAgent] ─── 選択肢ごとに条文と照合
   │                      
   │  選択肢a: 条文Xと比較 → 一致/不一致 + 根拠
   │  選択肢b: 条文Yと比較 → 一致/不一致 + 根拠
   │  選択肢c: 条文Zと比較 → 一致/不一致 + 根拠
   │  選択肢d: 条文Wと比較 → 一致/不一致 + 根拠
   │
   v
[JudgeAgent] ─── 検証結果を統合し最終判定
   │
   v
[最終回答]
```

### 期待効果

| 改善点 | 効果 |
|--------|------|
| 選択肢ごとの検証 | 見落とし防止 |
| 数値の明示的抽出・比較 | 細かい違いを検出 |
| 段階的推論 | 判断根拠の透明化 |

---

## 提案手法 v4 最終結果

### 実験概要

| 項目 | 値 |
|------|-----|
| 手法 | v4: 2-Agent (RetrieverAgent + IntegratorAgent) |
| インデックス | XML v2 (5,045条文) |
| 検索 | Hybrid (Vector + BM25) |
| モデル | qwen3:8b |
| 実行日 | 2025-12-02 |

### 最終結果

```
=== v4 vs ベースライン ===
v4:        103/140 (73.6%)
Baseline:   96/140 (68.6%)
改善:      +7問 (+5.0%)
```

### 範囲別比較

| 範囲 | v4 | Baseline | 改善 |
|------|-----|----------|------|
| Q1-Q30 | 19/30 (63.3%) | 13/30 (43.3%) | **+6問 (+20%)** |
| Q31-Q60 | 24/30 (80.0%) | 24/30 (80.0%) | 0問 |
| Q61-Q90 | 23/30 (76.7%) | 22/30 (73.3%) | +1問 |
| Q91-Q120 | 22/30 (73.3%) | 21/30 (70.0%) | +1問 |
| Q121-Q140 | 15/20 (75.0%) | 16/20 (80.0%) | -1問 |

### 変化パターン分析

| パターン | 問題数 | 意味 |
|----------|--------|------|
| 両方正解 | 82問 | 安定的に正解 |
| 両方不正解 | 23問 | 共通の弱点 |
| v4のみ正解 | 21問 | **改善** |
| BLのみ正解 | 14問 | 劣化 |

**ネット改善**: 21 - 14 = **+7問**

### v4の強み

1. **Q1-30で大幅改善 (+20%)**
   - ベースラインが苦手だった複雑な問題を解決
   - RetrieverAgentの分析 + IntegratorAgentの統合が効果的

2. **21問で新たに正解**
   - 改善: Q2, Q3, Q10, Q20, Q25, Q26, Q30, Q49, Q50, Q53, Q54, Q69, Q70, Q85, Q86...
   - 複雑な参照関係や条件比較を正しく処理

3. **2エージェント構成の効果**
   - RetrieverAgent: 検索・初期分析
   - IntegratorAgent: サポート・最終判断
   - 役割分担による安定した推論

### v4の弱み

1. **14問で劣化**
   - 劣化: Q13, Q33, Q48, Q51, Q58, Q65, Q67, Q78, Q118, Q120, Q126, Q131, Q136, Q139
   - 一部の単純問題で過剰処理の可能性

2. **Q121-140で-5%**
   - 後半の問題で若干の精度低下
   - 単純な問題への過適応?

### 共通弱点（23問）

両方不正解の問題: Q6, Q11, Q17, Q18, Q19, Q22, Q23, Q24, Q28, Q29, Q37, Q40, Q63, Q64, Q83, Q88, Q95, Q98, Q102, Q105, Q114, Q119, Q130

**共通弱点の特徴**:
- 複数条文にまたがる複雑な比較
- 数値の微妙な違いの判断
- 施行令等の参照関係

### 結論

| 指標 | 結果 |
|------|------|
| 全体精度 | **73.6%** (+5.0% vs Baseline) |
| Q1-30改善 | **+20%** (最大の貢献) |
| ネット改善 | **+7問** |
| 研究貢献 | 2エージェント構成による法令QA精度向上を実証 |

v4は、特に難易度の高いQ1-30セクションで大幅な改善を達成し、全体でベースラインを5%上回る結果を得た。

---

## v4の弱点分析と改善実験

### v4失敗パターン分析

| カテゴリ | 失敗数 | 割合 |
|----------|--------|------|
| その他 | 21問 | 57% |
| 正しい選択 | 10問 | 27% |
| 誤り選択 | 4問 | 11% |
| 期間・数値比較 | 1問 | 3% |
| 組み合わせ | 1問 | 3% |

### 追加機能テスト結果

v4に個別機能を追加した効果を検証:

| 手法 | Q1-10 | 結果 |
|------|-------|------|
| **v4 (オリジナル)** | **9/10 (90%)** | **最良** |
| v4.1 (+ Reranker) | 4/10 (40%) | -50% |
| v4.2 (+ 数値比較) | 6/10 (60%) | -30% |

### 分析結果

1. **Rerankerが逆効果**
   - Cross-Encoderが法令文書に最適化されていない
   - 検索結果の順序を悪化させる

2. **数値比較情報が逆効果**
   - LLMに追加情報を与えると混乱する
   - シンプルなプロンプトの方が効果的

3. **v4のシンプルさが強み**
   - 2エージェント構成が最適
   - 追加処理は精度を下げる

### 結論

v4（73.6%）が現時点での最良結果。追加機能はすべて逆効果。

**今後の改善方針**:
1. v4のプロンプト改善（「正しい選択」「その他」パターン対策）
2. より大きなLLMモデルの検討
3. 法令特化のRerankerのFine-tuning（長期）
