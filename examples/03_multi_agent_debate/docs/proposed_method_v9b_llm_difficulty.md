# 提案手法 v9b-L: LLM難易度判定 + 構造化法令QA

## 0. スライド用クイックアウトライン（10枚想定）
- **背景/課題**: 金商法など専門法令で Hallucination を避けつつ厳密判断が必要（数値/主体/法令種別のズレが致命傷）。
- **目的**: 4択140問で正解率80%以上を安定達成。
- **手法の進化**: v7（ルール難易度）→ v9（構造化DB + 検証Agent）→ v9b-L（構造化DB + v7統合 + **LLM難易度判定**）。
- **アーキテクチャ**: Hybrid検索 → 構造化法令DB（数値・参照抽出）→ 難易度判定Agent → SIMPLE / MODERATE / COMPLEX パス。
- **強化ポイント(v9b-L最新)**: 分析プロンプトに構造化サマリを明示投入し、数値・主体・法令種別を choice ごとに○/×/△でチェックする出力フォーマットを指示。
- **評価設定**: lawqa_jp 140問, モデル qwen3:8b, Hybrid (Faiss + BM25), top_k=30, num_ctx=16k。
- **結果サマリ（最高時点: v9b-L, 8b）**: 106/140 (75.7%), simple 85.2%, moderate 73.5%, complex 73.3%。直近の改変後は 70% 前後で推移しており、さらなる改善が課題。
- **誤答要因**: moderate 26問集中。数値/期間、対象範囲・主体、条文・法令種別の微差取り違え。
- **改善ロードマップ**: 構造化DBを活かした照合強化（数値/主体/法令種別の3観点）、Few-shot追加、大きいLLM(qwen3:14b)再評価。
- **実行/ログ**: `python examples/03_multi_agent_debate/run_proposed_v9.py --model qwen3:8b --top-k 30 --num-ctx 16000 --timeout 180`（結果JSONと `results/proposed_v9_run.log` を出力、バックグラウンド実行可）。

## 0.1 改善の流れ（スライド3枚想定）
- **v4 → v7**: ルールベース難易度判定 + マルチパスで過剰処理を抑止（76.4%）。
- **v9**: 構造化法令DBと検証Agentを導入するも、検証ステップが冗長化し 67.9% に低下。
- **v9b**: 検証Agentを削除し、v7方式の分析+統合へ回帰（75.0%）。
- **v9b-L（現行ベースライン）**: 難易度判定をLLM化し、構造化DBを活用した分析を強化（最高時点 75.7%）。直近の改変（サマリ注入・○/△/×指示など）後は 70% 前後に停滞しており、Few-shot強化と検索前処理の精緻化が次の焦点。

## 1. 研究背景・目的

日本の法令QAタスク（4択式・140問）では、以下の課題がある。

- **高い正確性要求**: 金融商品取引法など専門法令では、条文解釈の誤りが致命的。
- **複雑な条文構造**: 「政令で定める」「施行令」「施行規則」など他法令への参照が多い。
- **数値・期間条件の多さ**: 「三月以内」「六十日以内」「三分の二以上」等の厳密な比較が必要。
- **Hallucination問題**: LLMが根拠のない回答を生成するリスク。

本研究では、RAG（Retrieval-Augmented Generation）とマルチパス構成を組み合わせ、
**80%以上の正解率**を目標として、以下を満たす法令QAシステムを目指す。

- 条文を根拠とした厳密な判断
- 数値・期間条件の機械的な比較
- 問題の難易度に応じた処理パスの切り替え

その中でも本ドキュメントでは、

- 構造化法令DBを用いた v9b ワークフロー
- **LLMによる難易度判定Agent** を組み込んだ拡張版（v9b-L）

について、実験結果と失敗分析をまとめる。

---

## 2. 提案手法 v9b-L の概要

### 2.1 ベースライン: v7 / v9 / v9b

- **v7**
  - ルールベース難易度判定（SIMPLE / MODERATE / COMPLEX）
  - SIMPLE: 1回のLLM呼び出しで直接回答
  - MODERATE: 各選択肢の個別分析 + 統合
  - COMPLEX: 実験時点ではサンプルが少数

- **v9**
  - 構造化法令DB（数値・参照関係の抽出）を導入
  - 分析Agent + 検証Agentによる2段階判断
  - 結果として **67.9%** と大きく精度低下（検証Agentが逆効果）

- **v9b**
  - 検証Agentを削除し、v7 方式の「分析 + 統合」に戻す
  - 構造化DBは維持し、数値比較などで活用
  - 難易度判定は引き続き **ルールベース**（キーワード + 数値数）

### 2.2 v9b-L: LLM難易度判定Agentの導入

v9b-L では、難易度判定をルールベースから **LLMベースの判定Agent** に置き換えた。

```python
# 擬似コードイメージ
prompt = f"""あなたは法令QA問題の難易度を判定するエージェントです。

【問題】
{question}

【選択肢】
{choices_text}

【判定基準】
- SIMPLE: 条文を1つ読めば直接答えられる単純な問題
- MODERATE: 複数の条文を比較したり、細かい条件の違いを判断する必要がある問題
- COMPLEX: 複雑な参照関係（政令、施行令等）や、複数の法的概念の組み合わせが必要な問題

難易度を SIMPLE / MODERATE / COMPLEX のいずれか1つだけ出力してください。
"""

label = llm.invoke(prompt)
```

LLMのラベルに応じて、以下のパスを選択する。

- `DifficultyLevel.SIMPLE`  → `_simple_path` （シンプル直接回答）
- `DifficultyLevel.MODERATE` → `_analyze_with_structure` → `_integrate_analysis`
- `DifficultyLevel.COMPLEX`  → `_analyze_with_structure` → `_integrate_analysis`（complex扱い）

### 2.3 構造化法令DB + Hybrid検索

- **HybridRetriever (Vector + BM25, RRF)** により、関連条文を取得。
- `StructuredLawDB` で、取得条文から次を抽出・表形式化:
  - 期間・割合・金額などの **数値条件**
  - 参照先法令（施行令、規則 等）
  - キーワード
- moderate / complex パスでは、この構造化情報をプロンプトに埋め込み、
  条文と選択肢の対応付け・数値比較を行う。

### 2.4 処理手順とアーキテクチャ（v9b-L 最新）

1. **検索 (Hybrid)**: 質問+選択肢を正規化 → Vector & BM25 → RRF で上位 `top_k` を取得。  
2. **構造化DB構築**: 取得条文から数値・主体・参照を抽出し `StructuredLawDB` に格納。  
3. **難易度判定Agent (LLM)**: SIMPLE / MODERATE / COMPLEX を分類し、処理パスを決定。  
4. **数値対比表生成**: `format_numbers_table` で条文と各選択肢の数値/期間を一覧化。  
5. **構造化サマリ注入**: `get_structured_context()` で条文ID・数値・参照のサマリを生成し、分析プロンプトへ明示投入。  
6. **分析パス (moderate/complex)**:  
   - choice ごとに「条文番号/法令種別」「数値・期間一致」「対象範囲・主体」を ○/△/× で評価する出力フォーマットを指示。  
   - 具体根拠をテキストで残させ、統合ステップの判断材料を明確化。  
7. **統合**: 分析結果と 3観点（数値・期間 / 対象範囲・主体 / 法令種別）で整合する選択肢を選択。検証Agentは用いず1段で決定。  
8. **ロギング & 途中保存**: `proposed_v9_run.log` と `proposed_v9_results.json` に進捗・精度を逐次保存。`--start` で既存結果をリセットし再計算可。

---

## 3. 実験設定

- **データセット**: `lawqa_jp / selection.json`（4択・140問）
- **モデル**: `qwen3:8b`（Ollama, temperature=0）
- **インデックス**:
  - Vector: `faiss_index_xml_v2/vector`（5,045 docs）
  - BM25: `faiss_index_xml_v2/bm25`（5,045 docs, SudachiPy tokenizer）
- **評価指標**:
  - 全体正解率
  - 難易度別正解率（LLMが判定した難易度ラベル）

比較対象:

| 手法 | 特徴 | 全体精度 |
|------|------|----------|
| v7 | ルール難易度 + v7パス | 107/140 (**76.4%**) |
| v9 | 構造化DB + 検証Agent | 95/140 (**67.9%**) |
| v9b | 構造化DB + v7方式統合 + ルール難易度 | 105/140 (**75.0%**) |
| **v9b-L** | 構造化DB + v7方式統合 + **LLM難易度判定** | **106/140 (75.7%)** |

---

## 4. 実験結果 (v9b-L)

### 4.1 全体・難易度別精度

`results/proposed_v9_results.json` より:

- **全体**: 106 / 140 (**75.7%**)
- **難易度別（LLM判定）**:

| 難易度 | 正解 / 問題数 | 正解率 |
|--------|---------------|--------|
| simple | 23 / 27 | **85.2%** |
| moderate | 72 / 98 | **73.5%** |
| complex | 11 / 15 | **73.3%** |

### 4.2 旧版との比較

| 手法 | 全体 | simple | moderate | complex |
|------|------|--------|----------|---------|
| v7 | 107/140 (76.4%) | 80.2% | 56.2% | 33.3% |
| v9b（ルール難易度） | 105/140 (75.0%) | 81.0% | 65.5% | 45.5% |
| **v9b-L（LLM難易度）** | **106/140 (75.7%)** | 85.2% | 73.5% | 73.3% |

- **moderate / complex** が大きく改善している一方、
- **simple問題数が 100 → 27 に大きく減少**（LLMが多くをmoderateに寄せている）。

### 4.3 最新自動評価ログ（2025-12-06, qwen3:8b, `top_k=30`, `num_ctx=16k`）

- 実行コマンド: `python examples/03_multi_agent_debate/run_proposed_v9.py --model qwen3:8b --top-k 30 --num-ctx 16000 --timeout 180`
- ログ: `examples/03_multi_agent_debate/results/proposed_v9_run.log`
- JSON: `examples/03_multi_agent_debate/results/proposed_v9_results.json`
- 結果: **98 / 140 (70.0%)**
  - simple: 26 / 36 (72.2%)
  - moderate: 66 / 91 (72.5%)
  - complex: 6 / 13 (46.2%)
- 直近変更（構造化サマリ注入 & ○/△/×出力指示）後も 70% に留まるため、Few-shot追加やモデル強化（14b）による再評価が必要。

### 4.4 追加検証（8b固定、2025-12-06）

| 実験 | 変更点 | 全体 | simple | moderate | complex | 保存先 |
|------|--------|------|--------|----------|---------|--------|
| top_k=40 | 検索上位拡大のみ | 98/140 (70.0%) | 26/36 (72.2%) | 66/91 (72.5%) | 6/13 (46.2%) | `.../proposed_v9_results_topk40.json` |
| Few-shot追加 | 分析プロンプトに短いFew-shot例を挿入 | 98/140 (70.0%) | 26/36 (72.2%) | 66/91 (72.5%) | 6/13 (46.2%) | `.../proposed_v9_results_fewshot.json` |
| analysis抜粋1500 | 統合プロンプトの分析抜粋長を1500文字に短縮 | 98/140 (70.0%) | 26/36 (72.2%) | 66/91 (72.5%) | 6/13 (46.2%) | `.../proposed_v9_results_trim1500.json` |
| top_k=50 + weighted_rrf (BM25 0.6) | 上位拡大 + BM25寄り融合 | 98/140 (70.0%) | 26/36 (72.2%) | 66/91 (72.5%) | 6/13 (46.2%) | `.../proposed_v9_results_topk50_bm25heavy.json` |
| midprompt v1（サマリ1200 + Few-shot） | 構造化サマリ短＋カテゴリFS | 103/140 (73.6%) | 20/24 (83.3%) | 75/102 (73.5%) | 8/14 (57.1%) | `.../proposed_v9_results_midprompt.json` |
| midprompt v2（サマリ2000 + complex FS追加） | サマリ拡張＋FS1件追加 | 101/140 (72.1%) | 23/30 (76.7%) | 71/97 (73.2%) | 7/13 (53.8%) | `.../proposed_v9_results.json`（上書き） |
| midprompt v2 + top_k=40 | サマリ拡張＋FS＋top_k40 | 89/140 (63.6%) | 21/42 (50.0%) | 61/87 (70.1%) | 7/11 (63.6%) | `.../proposed_v9_run_midprompt_v2_topk40.out` |
| hint + top_k(simple30/moderate30/complex40), num_ctx=20000 | 難易度別ヒント + complexのみ検索拡大 | **102/140 (72.9%)** | 33/36 (91.7%) | 62/89 (69.7%) | 7/15 (46.7%) | `.../proposed_v9_run_hint_topk30.out` |

→ いずれも 70% にとどまり、特に complex が 46.2% に張り付いている。次段は Few-shot をカテゴリ別で強化、検索前処理の精緻化を検討。

---

## 5. 失敗分析

### 5.1 誤答分布

- 誤答合計: **34問**
- 難易度別誤答数（LLM判定）:

| 難易度 | 誤答数 |
|--------|--------|
| simple | 4 |
| moderate | **26** |
| complex | 4 |

→ **80% に届かなかった主因は「moderate誤答 26問」** である。

### 5.2 moderate誤答 26問のカテゴリ分類

簡易なルールベース分類で、問題文 + 選択肢を次の観点でスキャンした。

- **数値・期間**: 日数・月数・年数、期間条件、割合（%）、金額 等
- **対象範囲・主体**: 「のみ」「を含む」「を除く」「に限る」、内国会社 / 外国会社、取締役 / 使用人 等
- **例外・ただし書き**: 「ただし」「〜を要しない」「〜ないものとする」 等
- **条文・法令種別**: 条文番号、施行令 / 施行規則 / 政令 などの区別

分類結果:

| カテゴリ | 件数 | 比率 (26問中) | 典型例のイメージ |
|----------|------|----------------|--------------------|
| **数値・期間** | 10 | **38.5%** | 三月以内 vs 六月以内、15日 vs 10日 など |
| **対象範囲・主体** | 6 | 23.1% | 外国会社も含むか、取締役等のみか 等 |
| **例外・ただし書き** | 0 | 0% | （今回の自動分類では顕在化せず） |
| **条文・法令種別** | 7 | 26.9% | 法本体か施行令か、どの条文を根拠にすべきか |
| **その他** | 3 | 11.5% | 上記に明確に当てはまらないもの |

代表的な失敗パターン:

- **数値・期間**
  - 取得勧誘の条件、届出期限、公告期間などで
  - 正解は「一定期間を超える譲渡禁止が付された場合のみ対象」だが、
    モデルは「誰が対象か（取締役等のみか）」に引きずられ誤答する、など。

- **対象範囲・主体**
  - 「内国会社 / 外国会社」「取締役等 / それ以外」など、
    条文上の**主体の限定**を正しく読み取れず、
    一見もっともらしい選択肢を選んでしまう。

- **条文・法令種別**
  - 金商法 本体の条文か、施行令・施行規則か、
    どの条文を根拠とすべきかを取り違えて誤答する。

### 5.3 失敗原因の整理

1. **moderateパスへの集中**
   - LLM難易度判定により、多くの問題が `moderate` に分類され、
     98問中 26問を取りこぼしている。

2. **条文の微妙な差異に対する弱さ**
   - 数値条件（期限・割合）、対象範囲（のみ / を除く / に限る）、
     法令種別（施行令か本法か）といった**ごく細かい条件差**が誤答の主因。

3. **プロンプトの情報過多**
   - `_analyze_with_structure` + `_integrate_analysis` のプロンプトは長く、
     条文引用・数値表・選択肢分析をすべて1回でこなしている。
   - その中で、決定的な一文（ただし書き・例外条項など）を見落とすケースがあると推定される。

---

## 6. 今後の方針

LLM難易度判定は、以下の理由から **採用する方針** とする。

- moderate / complex 精度を大きく押し上げている（特に complex 73.3%）。
- simple 問題へは厳しめにフィルタされるが、その分 **simple 精度は 85.2% と高い**。

一方で、80% 達成には **moderate誤答 26問のさらなる削減** が必須である。

### 6.1 moderateパスの改善方向

1. **数値・期間専用のチェック強化**
   - 構造化DBから抽出した期間・割合情報に対し、
     - 「どの選択肢が条文と完全一致しているか」を表形式で比較させる
     - `_integrate_analysis` 内で「期限・割合が条文と完全一致するか」を明示的に問う

2. **対象範囲・主体の明示的比較**
   - プロンプト内で、次のようなステップを追加する:

     > (1) 条文上の主体（内国会社 / 外国会社 / 取締役等 など）を列挙する
     > 
     > (2) 各選択肢の主体と条文の主体が一致しているかを判定する

3. **条文・法令種別の取り違え防止**
   - 「法律第○条」と「施行令第○条」「施行規則第○条」の違いを、
     `_analyze_with_structure` のテンプレート内で再度強調する。

### 6.2 システム全体の改善方向

- **LLMの強化**: qwen3:14b など、より大きなモデルでの再評価。
- **検索チューニング**: v4/v7で得られた知見を元に、
  クエリ正規化やQuery Expansionをさらに洗練させる。
- **誤答パターン別のFew-shot例追加**:
  - 数値・期間
  - 対象範囲・主体
  - 条文・法令種別
  といったカテゴリごとに代表問題をFew-shotとしてプロンプトに組み込む。
- **難易度判定Agentによるプロンプト自動生成（検討中）**:
  - 難易度判定と同時に「その難易度に適した分析・統合プロンプト雛形」を生成させる案。
  - RAG連携はオプション。まずは非RAGで短い指示セットを生成し、分析/統合に挿入する軽量版から試す。

---

## 7. 現時点のまとめ

- **v9b-L（構造化DB + v7方式統合 + LLM難易度判定）** は、
  - v9b（ルール難易度）より **+0.7pt 改善 (75.7%)**
  - v7 (76.4%) とほぼ同水準まで到達
- 最も大きな課題は **moderate誤答 26問** であり、
  - 数値・期間
  - 対象範囲・主体
  - 条文・法令種別
  の3カテゴリで全体の約 9割を占める。

今後は、

1. LLM難易度判定を前提とした v9b-L を現行ベースラインとしつつ、
2. moderateパスのプロンプト・構造化情報の扱いを、上記3カテゴリに特化して改善し、
3. 80% 達成に向けた追加実験（LLM強化 / Few-shot 追加等）を段階的に実施する。

---

## 8. 方向性と現状（v9b-Lの追加実験結果と今後の方針）

### 8.1 v9b-L 統合方針強化版の追加実験

`_integrate_analysis` の統合プロンプトに対して、

- 数値・期間条件
- 対象範囲・主体
- 条文番号・法令種別

の3観点を明示的に重視する指示を追加した **v9b-L 試験版** を実験した。

その結果、140問テストでは次の成績となった。

- **全体**: 98 / 140 (**70.0%**)
- 難易度別（LLM判定）:

| 難易度 | 正解 / 問題数 | 正解率 |
|--------|---------------|--------|
| simple | 26 / 36 | 72.2% |
| moderate | 66 / 91 | 72.5% |
| complex | 6 / 13 | 46.2% |

比較:

- 旧 v9b-L（統合方針変更前）: **106/140 (75.7%)**
  - simple: 23/27 (85.2%) / moderate: 72/98 (73.5%) / complex: 11/15 (73.3%)
- 統合方針強化版 v9b-L: **98/140 (70.0%)**
  - 特に simple / complex の精度が大きく悪化

**結論**:

- `_integrate_analysis` における統合方針の強化（3観点の明示的な優先）は、
  - 期待したほど moderate 精度を押し上げず、
  - 既に正解していた simple / complex を崩してしまい、全体としては劣化した。
- このため、
  - 統合ステップは **従来のシンプルな形を維持** し、
  - 改善の主戦場は **`_analyze_with_structure` 側の出力構造（3観点の整理）** に移す方針とする。

### 8.2 現在のベースラインと改善方向

- **現行ベースライン**
  - 構造化DB + v7方式統合 + **LLM難易度判定**（旧 v9b-L、全体 75.7%）
  - `_integrate_analysis` は、分析結果を素直に統合するシンプルな形を維持する。

- **今後の改善の主なターゲット**
  - `_analyze_with_structure` のプロンプト・出力構造を、
    - 数値・期間
    - 対象範囲・主体
    - 条文・法令種別
    の3観点で整理し、各選択肢の「どこが条文とズレているか」をより明確にさせる。
  - その上で、必要に応じて Few-shot 例やより大きなLLMでの再評価を実施する。

---

## 9. 関連ドキュメント一覧

本ドキュメント（v9b-L: LLM難易度判定 + 構造化法令QA）と関連するドキュメントは次の通り。

- `examples/03_multi_agent_debate/docs/proposed_method_v7.md`
  - 難易度適応型マルチパス法令QA (**v7**) の提案手法・実験結果・失敗分析。
  - v7 の難易度判定ロジックや SIMPLE/MODERATE パスのプロンプト設計の詳細。

- `examples/03_multi_agent_debate/docs/proposed_method_v9_design.md`
  - v9 系列（構造化法令DB + マルチエージェント）の全体設計。
  - DifficultyJudgeAgent / AnalysisAgent / VerificationAgent などの役割とフロー。

- `examples/03_multi_agent_debate/docs/experiment_results.md`
  - ベースライン（Vector / Hybrid / Reranker）および v1〜v2 の実験結果まとめ。
  - 検索品質分析や、共通弱点問題の一覧など。

- `examples/03_multi_agent_debate/docs/future_improvements.md`
  - モデルサイズ拡大、温度調整、Few-shot追加、Fine-tuning など、
    中長期の改善案を整理したドキュメント。

- `examples/03_multi_agent_debate/docs/research_contribution.md`
  - 本研究全体としての貢献・位置づけ・今後の展望をまとめたドキュメント。

- `examples/03_multi_agent_debate/docs/improvement_log.md`
  - 実験ごとの改善履歴・変更点を時系列で記録したログ。

実装詳細については、次のコードも参照するとよい。

- `examples/03_multi_agent_debate/proposed_workflow_v9.py`
  - v9b/v9b-L ワークフロー本体（難易度判定、simple/moderate/complex パス）。
- `examples/03_multi_agent_debate/structured_law_db.py`
  - 構造化法令DBの実装（数値・参照関係の抽出・整形）。

---

## 10. 最終報告資料のスライド構成（15枚）

本セクションは、学会発表を想定したスライド原稿です。各スライドには **タイトル / 要点 / 原稿（読み上げ目安）** を記載しています。

---

### スライド 1: タイトル

**タイトル:** 構造化法令DB + 難易度適応マルチパスによる法令QAシステム  
**サブタイトル:** Retrieval-Augmented Generation と LLM 難易度判定を組み合わせた提案手法  

**要点（箇条書き）:**
- 発表者名・所属
- 日付

**原稿:**
「本発表では、日本語法令QAタスクにおいて80%の正解率を目標に、構造化法令DBと難易度適応型マルチパスを組み合わせた提案手法をご紹介します。」

---

### スライド 2: 研究背景

**タイトル:** 研究背景

**要点:**
- 法令QAタスクの重要性（金融・法務・行政のDX）
- 4択問題140問、金融商品取引法などの専門法令を対象
- 課題: 漢数字・参照関係・Hallucination

**原稿:**
「法令QAは、行政・金融・法務分野のDXで重要なタスクです。本研究では、金融商品取引法等を対象とした4択140問のデータセットを用います。日本語法令は漢数字表記や複雑な参照関係があり、一般的なRAGでは誤答しやすいという課題があります。」

---

### スライド 3: モチベーションと目的

**タイトル:** モチベーションと目的

**要点:**
- 既存RAGでは70%前後で頭打ち
- 80%以上の正解率を安定達成したい
- 誤答の主因: 数値・期間、対象範囲・主体、法令種別の微差

**原稿:**
「初期検討で、単純なRAGは70%前後で頭打ちになりました。誤答を分析すると、数値・期間、対象範囲・主体、法令種別の微妙な違いが原因でした。本研究の目的は、これらの誤答を削減し、80%以上の正解率を達成することです。」

---

### スライド 4: 関連研究と位置づけ

**タイトル:** 関連研究と本研究の位置づけ

**要点:**
- RAG（Retrieval-Augmented Generation）
- Multi-Agent QA / 難易度適応手法
- 本研究: 構造化DB + LLM難易度判定 + Hybrid検索の統合

**原稿:**
「関連研究として、RAGやマルチエージェントQAがあります。本研究は、構造化法令DBとLLMによる難易度判定を組み合わせ、Hybrid検索（Vector+BM25）の結果を活用して回答精度を高めます。」

---

### スライド 5: 手法の進化（v4→v7→v9b-L）

**タイトル:** 手法の進化

**要点:**
- v4: 単一パス + RAG → 73.6%
- v7: ルール難易度判定 + マルチパス → 76.4%
- v9: 構造化DB + 検証Agent → 67.9%（過剰検証で低下）
- v9b-L: 検証Agent撤廃 + LLM難易度判定 → **75.7%**（最高時点）

**原稿:**
「手法の進化を概観します。v4の単純RAGは73.6%。v7でルールベース難易度判定を導入し76.4%まで向上。v9で構造化DBと検証Agentを追加しましたが、過剰検証で67.9%に低下。v9b-Lでは検証Agentを外し、難易度判定をLLM化することで75.7%まで回復しました。」

---

### スライド 6: 提案手法の全体アーキテクチャ

**タイトル:** 提案手法アーキテクチャ（v9b-L）

**要点（図解を推奨）:**
```
質問+選択肢 → 正規化 → Hybrid検索(Vector+BM25)
    ↓
構造化法令DB構築（数値・参照抽出）
    ↓
難易度判定Agent (LLM) → SIMPLE / MODERATE / COMPLEX
    ├─ SIMPLE: 直接回答（1回LLM）
    └─ MODERATE/COMPLEX: 分析Agent → 統合
         └→ 最終回答（a/b/c/d）
```

**原稿:**
「アーキテクチャです。質問と選択肢を正規化し、Hybrid検索で条文を取得します。取得条文から構造化法令DB（数値・参照）を構築し、LLMが難易度を判定します。SIMPLEなら直接回答、それ以外は分析Agentで各選択肢を評価し、統合で最終回答を決定します。」

---

### スライド 7: 構造化法令DB

**タイトル:** 構造化法令DB

**要点:**
- 条文から数値（期間・割合・金額）を抽出
- 参照関係（施行令・規則・他条文）を抽出
- numbers_table / structured_summary として分析プロンプトに投入

**原稿:**
「構造化法令DBでは、条文から期間・割合・金額などの数値と、施行令や規則への参照関係を抽出します。これらをnumbers_tableやstructured_summaryとして分析プロンプトに埋め込み、条文と選択肢の対応付けを明確にします。」

---

### スライド 8: 難易度判定Agent（LLMベース）

**タイトル:** 難易度判定Agent

**要点:**
- LLMにSIMPLE/MODERATE/COMPLEXを判定させる
- SIMPLEは短プロンプトで即答（過剰処理を回避）
- MODERATE/COMPLEXは構造化分析パスへ

**原稿:**
「難易度判定AgentはLLMに問題の複雑さを判定させます。SIMPLEと判定されれば短いプロンプトで即答し、過剰処理を回避します。MODERATEやCOMPLEXなら、構造化分析パスへ送り、丁寧に判断します。」

---

### スライド 9: 分析Agent（○/△/×評価）

**タイトル:** 分析Agent

**要点:**
- 各選択肢について3観点でチェック:
  - 数値・期間の一致（○/△/×）
  - 対象範囲・主体の一致（○/△/×）
  - 条文番号・法令種別の一致（○/△/×）
- 根拠を短くテキストで残す

**原稿:**
「分析Agentでは、各選択肢を3つの観点でチェックします。数値・期間、対象範囲・主体、条文番号・法令種別が条文と一致しているかを○/△/×で評価し、根拠を短く残します。これにより、統合ステップで判断しやすくなります。」

---

### スライド 10: 統合と最終回答

**タイトル:** 統合と最終回答

**要点:**
- 分析結果を1500字程度にトリムして入力
- 検証Agentは用いず、1回のLLM呼び出しで最終決定
- 過剰検証による精度低下を回避（v9の教訓）

**原稿:**
「統合ステップでは、分析結果を要約して1回のLLM呼び出しで最終回答を決定します。v9で検証Agentを追加した際に精度が低下した教訓から、シンプルな統合を維持しています。」

---

### スライド 11: 評価実験の設定

**タイトル:** 評価実験設定

**要点:**
- データセット: lawqa_jp（4択140問）
- モデル: qwen3:8b（Ollama, temperature=0）
- 検索: Hybrid（FAISS + BM25, RRF）, top_k=30
- 評価指標: 全体正解率、難易度別正解率

**原稿:**
「評価実験の設定です。データセットはlawqa_jpの4択140問、モデルはqwen3:8b、検索はHybrid（FAISS+BM25, RRF）でtop_k=30です。全体正解率と難易度別の正解率を評価指標としました。」

---

### スライド 12: 実験結果

**タイトル:** 実験結果

**要点:**
| 手法 | 全体 | simple | moderate | complex |
|------|------|--------|----------|---------|
| v7 | 76.4% | 80.2% | 56.2% | 33.3% |
| v9（検証Agent有） | 67.9% | - | - | - |
| **v9b-L** | **75.7%** | 85.2% | 73.5% | 73.3% |

**原稿:**
「実験結果です。v7は76.4%、v9は検証Agentが逆効果で67.9%に低下。v9b-Lは検証Agentを外してLLM難易度判定を導入し、75.7%まで回復しました。特にmoderate（73.5%）とcomplex（73.3%）が大きく改善しています。」

---

### スライド 13: 失敗分析

**タイトル:** 失敗分析

**要点:**
- 誤答34問のうち、moderate 26問が集中
- 主な誤答カテゴリ:
  - 数値・期間（38.5%）
  - 対象範囲・主体（23.1%）
  - 条文・法令種別（26.9%）

**原稿:**
「失敗分析です。誤答34問のうち26問がmoderateに集中しています。カテゴリ別では、数値・期間が38.5%、対象範囲・主体が23.1%、条文・法令種別が26.9%を占めます。これらの微差を正確に判定することが今後の課題です。」

---

### スライド 14: 今後の改善方針

**タイトル:** 今後の改善方針

**要点:**
- カテゴリ別Few-shot強化（数値・期間 / 主体 / 法令種別）
- 検索前処理の精緻化（法令名・条番号正規化）
- 軽量チェックリスト検証（Yes/No形式）の追加検討
- top_k拡大とnum_ctx延伸（complex向け）

**原稿:**
「今後の改善方針です。カテゴリ別にFew-shotを強化し、検索前処理を精緻化します。また、軽量なチェックリスト検証や、complex向けのtop_k拡大も検討しています。」

---

### スライド 15: まとめ

**タイトル:** まとめ

**要点:**
- 構造化法令DB + LLM難易度判定 + Hybrid検索の統合手法を提案
- 最高時点 75.7%（v9b-L）、moderate/complex の大幅改善
- 80%達成に向け、Few-shot強化と検索精緻化が次の焦点

**原稿:**
「まとめです。本研究では、構造化法令DBとLLM難易度判定を組み合わせた法令QAシステムを提案しました。最高時点で75.7%を達成し、特にmoderateとcomplexの精度が大きく改善しました。80%達成に向け、Few-shot強化と検索精緻化を進めてまいります。ご清聴ありがとうございました。」

---

### 補足: 質疑応答用メモ

- **Q: 検証Agentを外した理由は？**
  A: v9で検証Agentを追加した際、過剰検証によりsimple問題を崩してしまい67.9%に低下。シンプルな統合に戻すことで75.7%まで回復した。

- **Q: モデルサイズを大きくすれば80%を超えるか？**
  A: 14b以上のモデルで再評価予定。ただし、Few-shot強化や検索精緻化の方が効果的な可能性もある。

- **Q: 他の法令データセットでも有効か？**
  A: 金融商品取引法以外の法令（民法・会社法等）での汎化性は今後の課題。

---

